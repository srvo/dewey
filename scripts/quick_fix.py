#!/usr/bin/env python3
"""
Quick Fix Tool for Pre-commit Issues.

This script helps interactively fix issues found by pre-commit hooks.
It parses issues from TODO.md (generated by capture_precommit_issues.py)
and provides various methods to fix them:

1. Interactive menu mode (default)
2. Batch mode (--batch)
3. LLM instruction generation (--generate-llm)
4. Gemini-powered analysis (--use-gemini)

For the Gemini analysis feature:
- Set the DEEPINFRA_API_KEY environment variable
- Run: python quick_fix.py --use-gemini
- The script will capture pre-commit output and send it to Gemini for analysis
- Results are written to gemini_precommit_analysis.md

Example usage:
  python quick_fix.py                      # Interactive menu
  python quick_fix.py --batch              # Process all issues
  python quick_fix.py --batch --category critical  # Process critical issues
  python quick_fix.py --generate-llm       # Generate LLM instructions
  python quick_fix.py --use-gemini         # Use Gemini to analyze raw output
"""

import argparse
import fnmatch
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import time
from typing import Any, Dict, List, Optional, Tuple

# Global settings
VERBOSE_MODE = False
INTERACTIVE_MODE = True
AUTO_ALL_MODE = False
GENERATE_LLM_INSTRUCTIONS = False
LLM_INSTRUCTION_FILE = "fix_instructions.md"
DEBUG_MODE = False  # More detailed debug output
MENU_SELECTIONS = []  # Global list to track menu selections
GENERATE_CODEBASE_INDEX = False  # Flag to generate codebase index
CHECK_IGNORED_FILES = False  # Flag to check ignored files

# Standard gitignore patterns to exclude
GITIGNORE_PATTERNS = [
    # Python
    "__pycache__/",
    "*.py[cod]",
    "*$py.class",
    "*.so",
    ".Python",
    "build/",
    "develop-eggs/",
    "dist/",
    "downloads/",
    "eggs/",
    ".eggs/",
    "lib/",
    "lib64/",
    "parts/",
    "sdist/",
    "var/",
    "wheels/",
    "*.egg-info/",
    ".installed.cfg",
    "*.egg",
    # Virtual Environment
    ".env",
    ".venv/",
    "venv/",
    "ENV/",
    # IDE
    ".idea/",
    ".vscode/",
    "*.swp",
    "*.swo",
    ".DS_Store",
    # Project specific
    "*.duckdb",
    "*.duckdb.wal",
    "backups/",
    "logs/",
    # Aider
    ".aider*",
    # Credentials and secrets
    "*_token.json",
    "*_credentials.json",
    "*_secret.*",
    "secrets/",
    "*_key.txt",
]

# Compiled regex patterns for faster matching
_COMPILED_IGNORE_PATTERNS = None

# Global variables
ISSUES_HEADER = "## Issues to Fix"
RESOLVED_HEADER = "## Resolved Issues"
WIP_HEADER = "## Work in Progress"
TODO_FILE = "TODO.md"
LLM_INSTRUCTION_FILE = "fix_instructions.md"
VERBOSE_MODE = False
DEBUG_MODE = False
INTERACTIVE_MODE = True
CLASS_INDEX = {}


def should_ignore_file(file_path: str) -> bool:
    """
    Check if a file should be ignored based on gitignore patterns.

    Args:
    ----
        file_path: Path to check

    Returns:
    -------
        True if the file should be ignored, False otherwise

    """
    global _COMPILED_IGNORE_PATTERNS

    # Initialize compiled patterns if needed
    if _COMPILED_IGNORE_PATTERNS is None:
        # Read actual .gitignore file if it exists
        gitignore_patterns = list(GITIGNORE_PATTERNS)
        if os.path.exists(".gitignore"):
            try:
                with open(".gitignore", encoding="utf-8") as f:
                    # Add any patterns not already in our standard list
                    for line in f:
                        line = line.strip()
                        # Skip empty lines and comments
                        if not line or line.startswith("#"):
                            continue
                        # Remove any trailing comments
                        if "#" in line:
                            line = line.split("#")[0].strip()
                        if line and line not in gitignore_patterns:
                            gitignore_patterns.append(line)
            except Exception as e:
                if VERBOSE_MODE:
                    print(colorize(f"Error reading .gitignore file: {e}", "1;33"))

        # Convert glob patterns to regex
        _COMPILED_IGNORE_PATTERNS = []
        for pattern in gitignore_patterns:
            if not pattern:
                continue

            # Handle directory-only patterns
            if pattern.endswith("/"):
                pattern = pattern + "**"

            # Convert glob pattern to regex pattern
            regex_pattern = fnmatch.translate(pattern)
            try:
                compiled = re.compile(regex_pattern)
                _COMPILED_IGNORE_PATTERNS.append(compiled)
            except Exception as e:
                if VERBOSE_MODE:
                    print(colorize(f"Error compiling pattern '{pattern}': {e}", "1;33"))

    # Check if file matches any patterns
    for pattern in _COMPILED_IGNORE_PATTERNS:
        if pattern.match(file_path):
            if VERBOSE_MODE:
                print(
                    colorize(
                        f"Ignoring '{file_path}' (matches gitignore pattern)", "1;33",
                    ),
                )
            return True

    # Additional check for common patterns that should be excluded
    if any(
        p in file_path
        for p in ["/logs/", "/backups/", "/venv/", "/.venv/", "/__pycache__/"]
    ):
        if VERBOSE_MODE:
            print(colorize(f"Ignoring '{file_path}' (in excluded directory)", "1;33"))
        return True

    return False


# Attempt to import Aider libraries - make this optional
try:
    from aider.coders import Coder
    from aider.io import InputOutput
    from aider.models import Model

    AIDER_AVAILABLE = True
except ImportError:
    AIDER_AVAILABLE = False

# TODO.md Section Headers
ISSUES_HEADER = "## Pre-commit Issues"
WIP_HEADER = "## Pre-commit WIP"
RESOLVED_HEADER = "## Pre-commit Resolved"
TODO_PATH = "TODO.md"


def colorize(text: str, color_code: str) -> str:
    """Add color to text."""
    return f"\033[{color_code}m{text}\033[0m"


def generate_codebase_index() -> bool:
    """
    Generate a fresh codebase_structure.txt file using repomix.

    Returns
    -------
        bool: True if the generation was successful, False otherwise

    """
    print(colorize("Generating fresh codebase index using repomix...", "1;36"))

    try:
        # Check if repomix is installed
        try:
            subprocess.run(["which", "repomix"], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            print(colorize("Repomix not found. Attempting to install...", "1;33"))
            subprocess.run(["npm", "install", "-g", "repomix"], check=True)

        # Run repomix to generate a fresh index with better formatting options
        # Use markdown style with explicit file headers for easier parsing
        cmd = [
            "repomix",
            "--output",
            "codebase_structure.txt",
            "--include",
            "**/*.py",
            "--style",
            "markdown",  # Use markdown format for clear file headers
            "--remove-comments",  # Remove comments to reduce file size
            "--parsable-style",  # Ensure consistent formatting that's easier to parse
            "--header-text",
            "# Python Classes Index",  # Add a clear header
        ]

        if VERBOSE_MODE:
            # Show output in real-time if verbose mode is enabled
            result = subprocess.run(cmd, check=False)
        else:
            # Otherwise, capture output to keep things clean
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        if result.returncode == 0:
            print(colorize("✅ Codebase index generated successfully!", "1;32"))

            # Add a supplementary class index for even faster lookups
            print(colorize("Building class-to-file index...", "1;36"))
            try:
                build_class_index()
                print(colorize("✅ Class index generated successfully!", "1;32"))
            except Exception as e:
                print(colorize(f"Warning: Could not build class index: {e}", "1;33"))
                print(colorize("Will rely on full-text search only", "1;33"))

            return True
        print(colorize("❌ Failed to generate codebase index.", "1;31"))
        if VERBOSE_MODE and not result.stdout:
            print(result.stdout)
        if result.stderr:
            print(colorize("Error:", "1;31"), result.stderr)
        return False

    except Exception as e:
        print(colorize(f"Error generating codebase index: {e}", "1;31"))
        return False


def build_class_index() -> None:
    """
    Create a supplementary index mapping class names to file paths.

    This makes lookups much faster by avoiding the need to search through
    the entire codebase_structure.txt file each time.
    """
    repomix_file = "codebase_structure.txt"
    class_index_file = "class_index.json"

    class_map = {}

    # Read the repomix output file
    with open(repomix_file, encoding="utf-8", errors="ignore") as f:
        content = f.read()

    # Extract file paths and class definitions
    file_blocks = re.split(r"(?:^|\n)#+\s+File:\s+", content)

    for block in file_blocks[1:]:  # Skip the first block (header)
        lines = block.strip().split("\n")
        if not lines:
            continue

        # Get the file path from the first line
        file_path = lines[0].strip()

        # Extract class definitions from this file
        class_matches = re.findall(r"class\s+(\w+)[\s:(]", block)

        # Add to the class map
        for class_name in class_matches:
            class_map[class_name] = file_path

    # Save the class map to a JSON file
    with open(class_index_file, "w", encoding="utf-8") as f:
        json.dump(class_map, f, indent=2)

    if VERBOSE_MODE:
        print(colorize(f"Created class index with {len(class_map)} entries", "1;32"))


def find_class_in_codebase(class_name: str) -> str | None:
    """
    Find the file containing the class definition.

    First checks the class_index.json file for fast lookups.
    Then checks the codebase_structure.txt file generated by repomix.
    Falls back to grep if neither works.

    Args:
    ----
        class_name: Name of the class to find

    Returns:
    -------
        File path if found, None otherwise

    """
    # First, check if we have a class index for instant lookups
    class_index_file = "class_index.json"
    if os.path.exists(class_index_file):
        try:
            with open(class_index_file, encoding="utf-8") as f:
                class_map = json.load(f)

            if class_name in class_map:
                file_path = class_map[class_name]

                # Verify the file exists and contains the class
                if os.path.exists(file_path):
                    print(
                        colorize(
                            f"Found class '{class_name}' in class index: {file_path}",
                            "1;32",
                        ),
                    )

                    # Double-check that the file actually contains the class
                    with open(file_path, encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                        class_pattern = re.compile(
                            rf"class\s+{re.escape(class_name)}[\s:(]",
                        )
                        if class_pattern.search(content):
                            return file_path
                        print(
                            colorize(
                                "Warning: Class not actually found in the file, continuing search...",
                                "1;33",
                            ),
                        )

                # Try with ./ prefix if needed
                elif not file_path.startswith("./") and os.path.exists(
                    f"./{file_path}",
                ):
                    file_path = f"./{file_path}"
                    print(
                        colorize(
                            f"Found class '{class_name}' in class index: {file_path}",
                            "1;32",
                        ),
                    )

                    # Double-check that the file actually contains the class
                    with open(file_path, encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                        class_pattern = re.compile(
                            rf"class\s+{re.escape(class_name)}[\s:(]",
                        )
                        if class_pattern.search(content):
                            return file_path
                        print(
                            colorize(
                                "Warning: Class not actually found in the file, continuing search...",
                                "1;33",
                            ),
                        )
        except Exception as e:
            if VERBOSE_MODE:
                print(colorize(f"Error reading class index: {e}", "1;33"))

    # Next, check if we have the repomix-generated file
    repomix_file = "codebase_structure.txt"

    if os.path.exists(repomix_file):
        print(
            colorize(
                f"Searching for class '{class_name}' in repository index...", "1;36",
            ),
        )
        try:
            # Use a more efficient pattern for searching large files
            pattern = re.compile(rf"class\s+{re.escape(class_name)}[\s:(]")

            if VERBOSE_MODE:
                print(colorize("Looking for repomix file format markers...", "1;30"))
                with open(repomix_file, encoding="utf-8", errors="ignore") as f:
                    first_lines = [next(f) for _ in range(20) if f]
                    print(colorize("First 20 lines of index file:", "1;30"))
                    for line in first_lines:
                        print(colorize(f"  {line.strip()}", "1;30"))

            # Open the file and search line by line with better format detection
            with open(repomix_file, encoding="utf-8", errors="ignore") as f:
                current_file = None
                current_section = None

                for line_number, line in enumerate(f, 1):
                    # Try multiple formats of file path indicators
                    if line.startswith("# File: "):
                        current_file = line[8:].strip()
                        if VERBOSE_MODE:
                            print(
                                colorize(f"Found file marker: {current_file}", "1;34"),
                            )
                    elif line.startswith("## File: "):
                        current_file = line[9:].strip()
                        if VERBOSE_MODE:
                            print(
                                colorize(
                                    f"Found alternate file marker: {current_file}",
                                    "1;34",
                                ),
                            )
                    elif "```" in line and ".py" in line:
                        # Look for markdown code blocks with filename
                        file_match = re.search(r"```(?:python:)?(.+?\.py)", line)
                        if file_match:
                            current_file = file_match.group(1).strip()
                            if VERBOSE_MODE:
                                print(
                                    colorize(
                                        f"Found markdown code block: {current_file}",
                                        "1;34",
                                    ),
                                )

                    # Look for section headings that might indicate file paths
                    elif line.startswith("# ") and ".py" in line:
                        section_match = re.search(r"#\s+(.+\.py)", line)
                        if section_match:
                            current_section = section_match.group(1).strip()
                            if VERBOSE_MODE:
                                print(
                                    colorize(
                                        f"Found section heading: {current_section}",
                                        "1;34",
                                    ),
                                )

                    # If we have a file path and found the class definition
                    if (current_file or current_section) and pattern.search(line):
                        found_file = current_file or current_section

                        # Try both absolute and relative paths
                        if found_file and os.path.exists(found_file):
                            print(
                                colorize(
                                    f"Found class '{class_name}' in index file: {found_file}",
                                    "1;32",
                                ),
                            )
                            return found_file

                        # Try with ./ prefix
                        if (
                            found_file
                            and not found_file.startswith("./")
                            and os.path.exists(f"./{found_file}")
                        ):
                            found_file = f"./{found_file}"
                            print(
                                colorize(
                                    f"Found class '{class_name}' in index file: {found_file}",
                                    "1;32",
                                ),
                            )
                            return found_file

                        # Try finding the file using grep if the path isn't valid
                        if found_file and not os.path.exists(found_file):
                            if VERBOSE_MODE:
                                print(
                                    colorize(
                                        f"Found class in index but path {found_file} doesn't exist, searching...",
                                        "1;33",
                                    ),
                                )

                            # Extract just the filename
                            filename = os.path.basename(found_file)
                            find_cmd = ["find", ".", "-type", "f", "-name", filename]
                            find_result = subprocess.run(
                                find_cmd, capture_output=True, text=True, check=False,
                            )

                            if find_result.returncode == 0 and find_result.stdout:
                                found_files = find_result.stdout.strip().split("\n")
                                if found_files:
                                    # Verify this is actually the right file by checking for the class
                                    for potential_file in found_files:
                                        with open(
                                            potential_file,
                                            encoding="utf-8",
                                            errors="ignore",
                                        ) as f:
                                            content = f.read()
                                            if pattern.search(content):
                                                print(
                                                    colorize(
                                                        f"Found class '{class_name}' in file: {potential_file}",
                                                        "1;32",
                                                    ),
                                                )
                                                return potential_file

                # Try one more approach - search for the class name anywhere in the file
                if VERBOSE_MODE:
                    print(
                        colorize(
                            "Class not found with file markers, doing full text search...",
                            "1;33",
                        ),
                    )

                # Reset file pointer to beginning
                with open(repomix_file, encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    class_matches = list(pattern.finditer(content))

                    if class_matches:
                        if VERBOSE_MODE:
                            print(
                                colorize(
                                    f"Found {len(class_matches)} matches for class '{class_name}' in index",
                                    "1;33",
                                ),
                            )

                        # For each match, try to find what file it belongs to by searching backward
                        for match in class_matches:
                            # Get the 5000 characters before the match to find file reference
                            start = max(0, match.start() - 5000)
                            before_text = content[start : match.start()]

                            # Look for file paths in the text before the match
                            file_candidates = re.findall(
                                r"(?:^|\n)(?:# File: |## File: |```(?:python:)?)([^\n]+\.py)",
                                before_text,
                            )

                            if file_candidates:
                                found_file = file_candidates[-1].strip()

                                # Verify file exists and contains the class
                                if os.path.exists(found_file):
                                    with open(
                                        found_file, encoding="utf-8", errors="ignore",
                                    ) as f:
                                        file_content = f.read()
                                        if pattern.search(file_content):
                                            print(
                                                colorize(
                                                    f"Found class '{class_name}' in index by context search: {found_file}",
                                                    "1;32",
                                                ),
                                            )
                                            return found_file
                                # Try with ./ prefix
                                elif not found_file.startswith("./") and os.path.exists(
                                    f"./{found_file}",
                                ):
                                    found_file = f"./{found_file}"
                                    with open(
                                        found_file, encoding="utf-8", errors="ignore",
                                    ) as f:
                                        file_content = f.read()
                                        if pattern.search(file_content):
                                            print(
                                                colorize(
                                                    f"Found class '{class_name}' in index by context search: {found_file}",
                                                    "1;32",
                                                ),
                                            )
                                            return found_file

            print(
                colorize(
                    f"Class '{class_name}' not found in index, trying grep fallback...",
                    "1;33",
                ),
            )
        except Exception as e:
            print(colorize(f"Error searching index file: {e}", "1;33"))
            print(colorize("Falling back to grep search...", "1;33"))
    else:
        print(colorize(f"Repository index not found at {repomix_file}", "1;33"))
        print(
            colorize(
                'Consider running: repomix --output codebase_structure.txt --include "**/*.py"',
                "1;33",
            ),
        )
        print(colorize("Falling back to grep search...", "1;33"))

    # Fallback to grep
    try:
        # Use grep to search for class definition
        cmd = ["grep", "-r", f"class {class_name}[(\\s:]", "--include=*.py", "."]
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        if result.returncode == 0 and result.stdout:
            # Parse the output to get the file path
            lines = result.stdout.strip().split("\n")
            for line in lines:
                # Extract file path from grep output (format: ./path/to/file.py:class ClassName...)
                parts = line.split(":", 1)
                if len(parts) >= 1 and parts[0].endswith(".py"):
                    file_path = parts[0]
                    # Verify this is actually a class definition, not a reference
                    with open(file_path) as f:
                        content = f.read()
                        class_pattern = re.compile(
                            rf"class\s+{re.escape(class_name)}[\s:(]",
                        )
                        if class_pattern.search(content):
                            print(
                                colorize(
                                    f"Found class '{class_name}' using grep: {file_path}",
                                    "1;32",
                                ),
                            )
                            return file_path

        print(colorize(f"Could not find class '{class_name}' in the codebase", "1;31"))
        return None
    except Exception as e:
        print(colorize(f"Error searching for class: {e}", "1;31"))
        return None


def read_todo_issues() -> list[dict[str, str]]:
    """Read pre-commit issues from the main Issues section in TODO.md."""
    todo_path = TODO_PATH
    print(colorize(f"[DEBUG] Reading issues from: {todo_path}", "1;35"))

    if not os.path.exists(todo_path):
        print(colorize(f"[DEBUG] Error: {todo_path} not found.", "1;31"))
        print(colorize("Error: TODO.md file not found.", "1;31"))
        return []

    try:
        with open(todo_path, encoding="utf-8") as f:
            content = f.read()
            # print(colorize(f"[DEBUG] Raw TODO.md content:\n---\n{content[:1000]}...\n---", "1;35")) # DEBUG (potentially too verbose)
    except Exception as e:
        print(colorize(f"[DEBUG] Error reading {todo_path}: {e}", "1;31"))
        return []

    # Find the specific pre-commit issues section, allowing for timestamp in header
    # Make pattern less greedy and more specific about newlines
    # Corrected the raw f-string definition to be on one logical line
    section_pattern = rf"^{re.escape(ISSUES_HEADER)}[^\n]*\n\n(.*?)(?=^## |\Z)"
    print(colorize(f"[DEBUG] Using regex pattern: {section_pattern}", "1;35"))
    section_match = re.search(section_pattern, content, re.MULTILINE | re.DOTALL)

    if not section_match:
        print(colorize(f"[DEBUG] Regex did not find section '{ISSUES_HEADER}'", "1;31"))
        print(colorize(f"No '{ISSUES_HEADER}' section found in {todo_path}.", "1;33"))
        return []
    print(colorize(f"[DEBUG] Regex FOUND section '{ISSUES_HEADER}'", "1;32"))

    issues_content = section_match.group(1).strip()
    print(
        colorize(f"[DEBUG] Extracted issues block:\n---\n{issues_content}\n---", "1;35"),
    )

    # Parse each issue
    issues = []
    consolidated_issues = {}  # For multi-issue entries

    lines = issues_content.split("\n")
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        print(colorize(f"[DEBUG] Processing line {i + 1}: '{line}'", "1;35"))
        i += 1

        if not line or not line.startswith("- "):
            print(
                colorize(
                    f"[DEBUG] Skipping line {i}: Not a valid issue format.", "1;35",
                ),
            )
            continue

        # Extract checkbox status
        checked = "[x]" in line or "[X]" in line
        if checked:
            continue  # Skip already fixed issues

        # Check for class missing execute method pattern
        # Updated regex to match the format written by capture_precommit_issues.py
        class_issue_pattern = r"Class '([^']+)' needs to implement '([^']+)' method(?: \\(in unknown file\\)| \\(required by BaseScript\\))?"
        class_execute_match = re.search(class_issue_pattern, line)
        if class_execute_match:
            class_name = class_execute_match.group(1)
            method_name = class_execute_match.group(2)
            issues.append(
                {
                    "line": line,
                    "file_path": "",  # No specific file path for class issues
                    "error_msg": f"Class '{class_name}' needs to implement '{method_name}' method",
                    "type": "class",
                    "checked": checked,
                    "consolidated": False,
                },
            )
            continue

        # Check if this is a consolidated issue (has sub-bullets)
        if "Fix issues in `" in line or "Fix issues in " in line:
            # Extract file path - support both backticked and non-backticked formats
            file_match = re.search(r"`([^`]+)`", line)
            if not file_match:
                # Try without backticks - look for "Fix issues in FILENAME:"
                file_match = re.search(r"Fix issues in ([^:]+):", line)

            if not file_match:
                if DEBUG_MODE:
                    print(
                        colorize(
                            f"[DEBUG] Skipping line {i}: Consolidated issue, but no file path found.",
                            "1;35",
                        ),
                    )
                continue

            file_path = file_match.group(1)
            if DEBUG_MODE:
                print(
                    colorize(
                        f"[DEBUG] Found consolidated issue for file: {file_path}", "1;35",
                    ),
                )

            # Find all sub-issues (indented bullets)
            sub_issues = []
            while i < len(lines) and (
                lines[i].strip().startswith("  -") or lines[i].strip().startswith(" -")
            ):
                sub_issue = lines[i].strip()
                if sub_issue.startswith("  -"):
                    sub_issue = sub_issue[3:].strip()  # Remove "  - " prefix
                elif sub_issue.startswith(" -"):
                    sub_issue = sub_issue[2:].strip()  # Remove " - " prefix
                sub_issues.append(sub_issue)
                i += 1

            issues.append(
                {
                    "line": line,
                    "file_path": file_path,
                    "error_msg": "\n".join(sub_issues),
                    "type": "file",
                    "checked": checked,
                    "consolidated": True,
                },
            )
            continue

        # Handle different issue formats: both with and without backticks
        file_match = re.search(r"`([^`]+)`", line)
        code_match = None
        if not file_match:
            # Try to match other formats without backticks

            # Format 1: filename.py: Error message
            filename_match = re.search(
                r"([\w\/\-\.]+\.(py|js|json|yaml|yml|md|rst)): (.+)", line,
            )
            if filename_match:
                first_part = filename_match.group(1)
                error_msg = filename_match.group(3)

                issues.append(
                    {
                        "line": line,
                        "file_path": first_part,
                        "error_msg": error_msg,
                        "type": "file",
                        "checked": checked,
                        "consolidated": False,
                    },
                )
                continue

            # Format 2: Common code errors like D100, G004, etc.
            for code in [
                "D100",
                "D101",
                "D102",
                "D103",
                "D213",
                "G004",
                "TRY400",
                "BLE001",
                "S101",
            ]:
                if code in line:
                    code_match = code
                    break

            if code_match:
                if DEBUG_MODE:
                    print(colorize(f"[DEBUG] Found issue with code: {code_match}", "1;35"))
                issues.append(
                    {
                        "line": line,
                        "file_path": code_match,  # Use the code as a hook
                        "error_msg": line.replace("- [ ] ", ""),
                        "type": "hook",
                        "checked": checked,
                        "consolidated": False,
                    },
                )
                continue

            if DEBUG_MODE:
                print(
                    colorize(
                        f"[DEBUG] Skipping line {i}: Single issue, but no backticked path/hook or recognizable format found.",
                        "1;35",
                    ),
                )
            continue

        first_part = file_match.group(1)
        if DEBUG_MODE:
            print(colorize(f"[DEBUG] Found single issue part: {first_part}", "1;35"))

        # Handle both formats: file paths and hook names
        if os.path.exists(first_part) or first_part.endswith(
            (".py", ".js", ".json", ".yaml", ".yml", ".md", ".rst"),
        ):
            # It's a file path
            file_path = first_part
            remaining = line.split(f"`{file_path}`", 1)[1].strip()
            if remaining.startswith(": "):
                remaining = remaining[2:]
            error_msg = remaining
            issue_type = "file"
        else:
            # It's a hook name or code
            hook_name = first_part
            remaining = line.split(f"`{hook_name}`", 1)[1].strip()
            if remaining.startswith(" failure: "):
                remaining = remaining[10:]
            error_msg = remaining
            file_path = ""  # No specific file for hook failures
            issue_type = "hook"
            if DEBUG_MODE:
                print(colorize(f"[DEBUG] Parsed as HOOK issue: {hook_name}", "1;35"))

        issues.append(
            {
                "line": line,
                "file_path": file_path,
                "error_msg": error_msg,
                "type": issue_type,
                "checked": checked,
                "consolidated": False,
            },
        )

    if DEBUG_MODE:
        print(colorize(f"[DEBUG] Finished processing. Found {len(issues)} issues.", "1;35"))
        # print(colorize(f"[DEBUG] Final issues list: {issues}", "1;35")) # DEBUG (Potentially verbose)
    return issues


def ensure_todo_sections() -> None:
    """Ensure WIP and Resolved sections exist in TODO.md."""
    if not os.path.exists(TODO_PATH):
        print(colorize(f"{TODO_PATH} not found. Cannot ensure sections.", "1;31"))
        return

    with open(TODO_PATH) as f:
        content = f.read()

    needs_update = False
    if WIP_HEADER not in content:
        content = (
            content.rstrip()
            + f"\n\n{WIP_HEADER}\n\n(No issues currently in progress)\n"
        )
        needs_update = True
        print(colorize(f"Added '{WIP_HEADER}' section to {TODO_PATH}", "1;32"))

    if RESOLVED_HEADER not in content:
        content = (
            content.rstrip()
            + f"\n\n{RESOLVED_HEADER}\n\n(No issues resolved yet in this run)\n"
        )
        needs_update = True
        print(colorize(f"Added '{RESOLVED_HEADER}' section to {TODO_PATH}", "1;32"))

    if needs_update:
        content = content.rstrip() + "\n"  # Ensure single trailing newline
        try:
            # Atomic write
            with tempfile.NamedTemporaryFile(
                mode="w", delete=False, dir=os.path.dirname(TODO_PATH),
            ) as temp_file:
                temp_file.write(content)
                temp_path = temp_file.name
            shutil.move(temp_path, TODO_PATH)
        except Exception as e:
            print(
                colorize(f"Error updating {TODO_PATH} with new sections: {e}", "1;31"),
            )
            if "temp_path" in locals() and os.path.exists(temp_path):
                os.remove(temp_path)


def move_todo_issue(issue_line: str, source_header: str, target_header: str) -> bool:
    """Move an issue line from one section to another in TODO.md."""
    if not issue_line:
        print(colorize("Cannot move empty issue line", "1;33"))
        return False

    issue_line_stripped = issue_line.strip()
    if not issue_line_stripped:
        return False

    try:
        with open(TODO_PATH, encoding="utf-8") as f:  # Ensure encoding
            content = f.read()

        # --- Find Source Section Content using Regex (Consistent with read_todo_issues) ---
        source_section_pattern = (
            rf"^{re.escape(source_header)}[^\n]*\n\n(.*?)(?=^## |\Z)"
        )
        source_section_match = re.search(
            source_section_pattern, content, re.MULTILINE | re.DOTALL,
        )

        if not source_section_match:
            print(
                colorize(
                    f"[MoveIssue DEBUG] Regex failed to find source section '{source_header}'",
                    "1;31",
                ),
            )
            return False

        source_section_content = source_section_match.group(1)
        # Store the start/end indices of the content block itself for later reconstruction
        source_content_start = source_section_match.start(1)
        source_content_end = source_section_match.end(1)
        print(
            colorize(
                f"[MoveIssue DEBUG] Source '{source_header}' content block (indices {source_content_start}-{source_content_end}) found.",
                "1;35",
            ),
        )
        # ----------------------------------------------------------------------------------

        # Find the exact issue line within the extracted source section content
        lines_in_source = source_section_content.splitlines(keepends=True)
        found_line_index = -1
        original_line_to_move = ""
        for i, line in enumerate(lines_in_source):
            if line.strip() == issue_line_stripped:
                found_line_index = i
                original_line_to_move = (
                    line  # Keep the original line with its whitespace/newline
                )
                print(
                    colorize(
                        f"[MoveIssue DEBUG] Found line '{issue_line_stripped}' at index {i} in source section.",
                        "1;35",
                    ),
                )
                break

        if found_line_index == -1:
            print(
                colorize(
                    f"[MoveIssue DEBUG] Issue line not found in extracted content of '{source_header}'. Searched for: '{issue_line_stripped}'",
                    "1;33",
                ),
            )
            # print(colorize(f"Content searched:\n{source_section_content}", "1;30")) # Optional further debug
            print(
                colorize(
                    f"Issue line not found in '{source_header}': {issue_line_stripped}",
                    "1;33",
                ),
            )
            return False

        # --- Find Target Section for Insertion Point ---
        target_section_pattern = (
            rf"(^({re.escape(target_header)}[^\n]*)\n\n)(.*?)(?=^## |\Z)"
        )
        target_section_match = re.search(
            target_section_pattern, content, re.MULTILINE | re.DOTALL,
        )
        if not target_section_match:
            print(
                colorize(
                    f"[MoveIssue DEBUG] Regex failed to find target section '{target_header}'",
                    "1;31",
                ),
            )
            # Attempt to add the section if missing (might be better in ensure_sections)
            # ensure_todo_sections() # Re-ensure might help, or handle append case
            # return False # Or try appending to end?
            # For now, fail if target section is gone
            return False

        target_header_full_line = target_section_match.group(
            2,
        )  # e.g. ## Pre-commit WIP (Timestamp...)
        target_content_start = target_section_match.start(3)
        target_content_end = target_section_match.end(3)
        target_current_content = target_section_match.group(3)
        print(
            colorize(
                f"[MoveIssue DEBUG] Target '{target_header}' content block (indices {target_content_start}-{target_content_end}) found.",
                "1;35",
            ),
        )
        # -------------------------------------------------

        # --- Prepare the updated content ---
        # Line to move (keep original whitespace/newline from source)
        line_to_insert = original_line_to_move

        # Remove the line from the source content block
        updated_source_lines = (
            lines_in_source[:found_line_index] + lines_in_source[found_line_index + 1 :]
        )
        updated_source_block = "".join(updated_source_lines)

        # Prepare the target content block
        target_lines = target_current_content.splitlines(keepends=True)
        # Remove placeholder if present
        if len(target_lines) == 1 and "(No issues" in target_lines[0]:
            target_lines = []
        # Insert the new line (e.g., at the beginning or end? Let's append)
        updated_target_lines = target_lines + [line_to_insert]
        updated_target_block = "".join(updated_target_lines)

        # Reconstruct the whole file content - more carefully this time
        # We need to replace the *content* part of the source and target sections

        # Content before source content block + updated source block +
        # Content between source and target content blocks + updated target block +
        # Content after target content block

        # This is still tricky. Let's try direct string replacement on the original content
        # assuming the indices are correct and don't overlap badly

        part1 = content[:source_content_start]
        part2_source = updated_source_block
        part3_between = content[source_content_end:target_content_start]
        part4_target = updated_target_block
        part5_after = content[target_content_end:]

        # Check indices make sense
        if not (
            source_content_start
            <= source_content_end
            <= target_content_start
            <= target_content_end
        ):
            # If sections are not ordered as expected, this direct slicing won't work
            # This might happen if WIP comes before Issues, etc.
            # Fallback to the previous complex reconstruction (or improve it)
            print(
                colorize(
                    f"[MoveIssue DEBUG] Section indices out of order, falling back may be needed. S:{source_content_start}-{source_content_end}, T:{target_content_start}-{target_content_end}",
                    "1;31",
                ),
            )
            # Reverting to previous reconstruction logic for now as it handled different orders
            # TODO: Improve the reconstruction logic below if needed
            # --- Previous reconstruction (potentially buggy) ---
            headers = [ISSUES_HEADER, WIP_HEADER, RESOLVED_HEADER]
            section_map = {}
            for header in headers:
                match = re.search(
                    rf"^{re.escape(header)}.*?\n\n(.*?)(?=^## |\Z)",
                    content,
                    re.MULTILINE | re.DOTALL,
                )
                if match:
                    section_map[header] = match.group(1)
                else:
                    section_map[header] = ""

            section_map[source_header] = updated_source_block
            target_content_current = section_map[target_header].strip()
            if "(No issues" in target_content_current and ")" in target_content_current:
                target_content_current = ""
            section_map[target_header] = (
                target_content_current + "\n" + line_to_insert.strip()
            ).strip() + "\n"

            first_section_match = re.search(
                r"^(## Pre-commit Issues|## Pre-commit WIP|## Pre-commit Resolved)",
                content,
                re.MULTILINE,
            )
            content_before = (
                content[: first_section_match.start()]
                if first_section_match
                else content
            )
            new_content = content_before.rstrip() + "\n\n"
            for header in [ISSUES_HEADER, WIP_HEADER, RESOLVED_HEADER]:
                header_line = next(
                    (line for line in content.splitlines() if line.startswith(header)),
                    header,
                )
                new_content += header_line + "\n\n"
                section_body = section_map.get(header, "").strip()
                if not section_body:
                    placeholder = ""
                    if header == WIP_HEADER:
                        placeholder = "(No issues currently in progress)"
                    elif header == RESOLVED_HEADER:
                        placeholder = "(No issues resolved yet in this run)"
                    elif header == ISSUES_HEADER:
                        placeholder = "(No issues detected)"
                    section_body = placeholder
                new_content += section_body + "\n\n"
            # --- End of Previous Reconstruction ---
        else:  # If indices are ordered correctly, use the simpler slicing approach
            new_content = (
                part1 + part2_source + part3_between + part4_target + part5_after
            )

        # Final cleanup for newlines
        new_content = new_content.rstrip() + "\n"  # Ensure single trailing newline

        # Atomic write
        with tempfile.NamedTemporaryFile(
            mode="w", delete=False, dir=os.path.dirname(TODO_PATH), encoding="utf-8",
        ) as temp_file:
            temp_file.write(new_content)
            temp_path = temp_file.name
        shutil.move(temp_path, TODO_PATH)
        print(
            colorize(f"Moved issue from '{source_header}' to '{target_header}'", "1;32"),
        )
        return True

    except Exception as e:
        # Add more specific exception details if possible
        import traceback

        print(
            colorize(
                f"Error moving issue in {TODO_PATH}: {e}\n{traceback.format_exc()}",
                "1;31",
            ),
        )
        if "temp_path" in locals() and os.path.exists(temp_path):
            os.remove(temp_path)
        return False


def preprocess_syntax_errors(file_path: str, error_msg: str) -> bool:
    """
    Attempt to fix severe syntax errors before running Aider.

    Handles common syntax errors like missing indented blocks
    that prevent the file from being parsed.

    Args:
    ----
        file_path: Path to the file to fix
        error_msg: Error message with details about the syntax error

    Returns:
    -------
        True if preprocessing was successful, False otherwise

    """
    if "expected an indented block" not in error_msg.lower():
        # Only handle indentation errors for now
        return False

    try:
        # Extract the line number from the error message
        line_match = re.search(r"on line (\d+)", error_msg)
        if not line_match:
            if VERBOSE_MODE:
                print(
                    colorize("Could not extract line number from error message", "1;33"),
                )
            return False

        line_num = int(line_match.group(1))

        # Read the file
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()

        # Check if the file has enough lines
        if line_num >= len(lines):
            if VERBOSE_MODE:
                print(colorize(f"Line number {line_num} is out of range", "1;33"))
            return False

        # Add a simple placeholder indented block if needed
        if line_num < len(lines) and not lines[line_num].strip():
            # There's already an empty line, let's add basic indentation
            lines[line_num] = "    pass  # Placeholder added by quick_fix.py\n"
        else:
            # Insert a new indented pass statement
            lines.insert(line_num, "    pass  # Placeholder added by quick_fix.py\n")

        # Write the file back
        with open(file_path, "w", encoding="utf-8") as f:
            f.writelines(lines)

        print(colorize(f"✅ Added placeholder indentation at line {line_num}", "1;32"))
        return True

    except Exception as e:
        print(colorize(f"Error preprocessing syntax errors: {e}", "1;31"))
        return False


def postprocess_with_ruff(file_path: str) -> bool:
    """
    Apply ruff formatting to a file after Aider has run.

    Args:
    ----
        file_path: Path to the file to format

    Returns:
    -------
        True if formatting was successful, False otherwise

    """
    try:
        # Check if ruff is installed
        try:
            subprocess.run(["which", "ruff"], check=True, capture_output=True)
            ruff_available = True
        except subprocess.CalledProcessError:
            ruff_available = False

        if not ruff_available:
            print(colorize("\nRuff formatter not found in your PATH.", "1;33"))

            # Offer to install ruff
            install_prompt = (
                "Would you like to install ruff for better code formatting? (y/n): "
            )
            if INTERACTIVE_MODE:
                if input(colorize(install_prompt, "1;33")).lower() == "y":
                    print(colorize("Installing ruff...", "1;34"))
                    try:
                        # Try to install using pip
                        subprocess.run(["pip", "install", "ruff"], check=True)
                        print(colorize("Ruff installed successfully!", "1;32"))
                        ruff_available = True
                    except Exception as e:
                        print(colorize(f"Error installing ruff: {e}", "1;31"))
                        return False
                else:
                    print(colorize("Skipping ruff formatting step.", "1;33"))
                    return False
            else:
                print(
                    colorize(
                        "Run with --interactive to install ruff, or install manually with: pip install ruff",
                        "1;33",
                    ),
                )
                return False

        print(colorize(f"Running ruff formatter on {file_path}...", "1;34"))

        # Try to run ruff format on the file
        result = subprocess.run(
            ["ruff", "format", file_path], capture_output=True, text=True, check=False,
        )

        format_success = result.returncode == 0
        if format_success:
            print(colorize("✅ Ruff formatting successful", "1;32"))
        elif VERBOSE_MODE:
            print(colorize("Ruff formatter failed:", "1;33"))
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                print(result.stderr)

        # Try running ruff with the --unsafe-fixes option for more aggressive fixes
        # Always try the unsafe fixes to handle the ~3,393 hidden fixes
        print(colorize("Applying ruff with unsafe fixes...", "1;33"))
        result = subprocess.run(
            [
                "ruff",
                "check",
                "--preview",  # Match pre-commit settings
                "--select=ALL",
                "--ignore=E501",
                "--ignore=E203",
                "--ignore=D203",
                "--ignore=D212",
                "--fix",
                "--unsafe-fixes",
                file_path,
            ],
            capture_output=True,
            text=True,
            check=False,
        )

        unsafe_success = result.returncode == 0
        if unsafe_success:
            print(colorize("✅ Ruff fixes with unsafe mode successful", "1;32"))
            return True
        if VERBOSE_MODE and (result.stdout or result.stderr):
            print(colorize("Ruff unsafe fixes failed:", "1;33"))
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                print(result.stderr)

        # Return success if either formatting or fixing worked
        return format_success or unsafe_success

    except Exception as e:
        print(colorize(f"Error running ruff formatter: {e}", "1;31"))
        return False


def setup_pre_commit_environment():
    """
    Set up environment for smoother interaction with pre-commit hooks.

    Instead of disabling hooks completely, this sets environment variables
    that control pre-commit behavior to make it work with our workflow.

    Returns
    -------
        A function that restores the original environment when called

    """
    # Store original environment variables to restore later
    original_env = {}

    # Environment variables to set for pre-commit
    env_vars = {
        # Prevent pre-commit from making changes in our files
        "PRE_COMMIT_NO_WRITE": "1",
        # Tell pre-commit this is not an interactive session
        "PRE_COMMIT_COLOR": "never",
        # Limit verbosity
        "PRE_COMMIT_QUIET": "1" if not VERBOSE_MODE else "0",
    }

    # Save original values and set new ones
    for key, value in env_vars.items():
        if key in os.environ:
            original_env[key] = os.environ[key]
        os.environ[key] = value

    if VERBOSE_MODE:
        print(
            colorize(
                "Pre-commit environment configured for controlled execution", "1;32",
            ),
        )

    def restore_environment():
        """Restore original environment variables."""
        for key in env_vars:
            if key in original_env:
                os.environ[key] = original_env[key]
            elif key in os.environ:
                del os.environ[key]

        if VERBOSE_MODE:
            print(colorize("Original environment restored", "1;32"))

    return restore_environment


def validate_fix(file_path: str) -> bool:
    """
    Validate that a file passes linting after a fix is applied.

    This uses the same configuration as pre-commit to ensure consistency.

    Args:
    ----
        file_path: Path to the file to validate

    Returns:
    -------
        True if validation passed, False otherwise

    """
    if not os.path.exists(file_path):
        return False

    try:
        # Run ruff with the same settings as pre-commit
        cmd = [
            "ruff",
            "check",
            "--preview",  # Match pre-commit config exactly
            "--select=ALL",
            "--ignore=E501",
            "--ignore=E203",
            "--ignore=D203",
            "--ignore=D212",
            file_path,
        ]

        if VERBOSE_MODE:
            print(colorize(f"Validating fix with: {' '.join(cmd)}", "1;36"))

        result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        # If return code is 0, no issues were found
        if result.returncode == 0:
            if VERBOSE_MODE:
                print(
                    colorize("Validation successful - no linting issues found", "1;32"),
                )
            return True

        # If issues were found, print them in verbose mode
        if VERBOSE_MODE:
            print(colorize("Validation failed - linting issues remain:", "1;33"))
            if result.stdout:
                print(colorize(result.stdout, "1;37"))

        return False
    except Exception as e:
        print(colorize(f"Error during validation: {e}", "1;31"))
        return False


def run_aider(
    file_path: str | None,  # Allow file_path to be None
    error_msg: str,
    issue: dict[str, Any] | None = None,
    consolidated: bool = False,
    class_name: str | None = None,
) -> bool:
    """
    Run aider on a file or with a general prompt to fix an issue.

    Args:
    ----
        file_path: Path to the file to fix (can be None for general issues)
        error_msg: Error message to fix
        issue: Original issue dictionary
        consolidated: Whether this is a consolidated issue
        class_name: Class name if this is a class issue

    Returns:
    -------
        True if the fix was successful, False otherwise

    """
    if not AIDER_AVAILABLE:
        print(
            colorize(
                "Aider is not available. Install with: uv pip install aider-chat",
                "1;31",
            ),
        )
        return False

    # Track if the issue has been moved to prevent duplicates
    issue_moved = False
    target_files = []
    prompt_context = ""
    issue_type = issue.get("type", "file") if issue else "file"

    # --- Determine target files and context ---
    if file_path:
        target_files.append(file_path)
        prompt_context = f" in {file_path}"
    elif issue and issue["type"] == "class" and class_name:
        # Special case for class issues where file wasn't found initially
        target_file = find_class_in_codebase(class_name)
        if target_file:
            target_files.append(target_file)
            prompt_context = f" in {target_file} (class {class_name})"
            print(
                colorize(f"Located class '{class_name}' in file: {target_file}", "1;32"),
            )
        else:
            print(
                colorize(
                    f"Could not find file for class '{class_name}' to fix the issue.",
                    "1;33",
                ),
            )
            return False  # Cannot proceed without a file for class implementation
    elif issue and issue["type"] == "hook":
        # Try to determine relevant files for hook issues
        relevant_files = []

        # Look for file patterns in the error message
        file_pattern = re.compile(r"([a-zA-Z0-9_\-\/]+\.py)")
        file_matches = file_pattern.findall(error_msg)

        # Try each potential file path
        for potential_file in file_matches:
            if os.path.exists(potential_file):
                relevant_files.append(potential_file)
            elif os.path.exists(f"./{potential_file}"):
                relevant_files.append(f"./{potential_file}")

        # Check for common error codes and search for them
        error_codes = [
            "D213",
            "G004",
            "TRY400",
            "BLE001",
            "D100",
            "D101",
            "D102",
            "D103",
            "I001",
            "S101",
            "ANN",
        ]
        code_in_error = None
        for code in error_codes:
            if code in error_msg:
                code_in_error = code
                break

        if code_in_error and not relevant_files:
            # Try to find instances of this error in the codebase
            try:
                # Run grep to find files with this error
                cmd = ["grep", "-r", "--include='*.py'", code_in_error, "."]
                result = subprocess.run(
                    cmd, capture_output=True, text=True, check=False,
                )

                if result.returncode == 0 and result.stdout:
                    # Extract file paths from grep results
                    lines = result.stdout.strip().split("\n")
                    for line in lines[:3]:  # Limit to first 3 files
                        file_path = line.split(":")[0]
                        if (
                            os.path.exists(file_path)
                            and file_path not in relevant_files
                        ):
                            relevant_files.append(file_path)
            except Exception as e:
                if VERBOSE_MODE:
                    print(
                        colorize(
                            f"Error searching for files with {code_in_error}: {e}",
                            "1;33",
                        ),
                    )

        if relevant_files:
            # Use the found files
            target_files = relevant_files[:1]  # Start with just one file
            prompt_context = f" in {target_files[0]} (and possibly other files)"
            print(
                colorize(
                    f"Found relevant file for hook issue: {target_files[0]}", "1;32",
                ),
            )
        else:
            # Initial file search failed, try re-running ruff for specific code
            hook_name = issue.get("file_path", "") # file_path holds the hook name/code for hook issues
            extracted_code = None
            code_match = re.search(r"([A-Z]{1,4}\d{3})", hook_name)
            if code_match:
                extracted_code = code_match.group(1)
            else: # Fallback: check error message if code not in hook_name
                code_match = re.search(r"([A-Z]{1,4}\d{3})", error_msg)
                if code_match:
                    extracted_code = code_match.group(1)

            if extracted_code:
                print(colorize(f"[DEBUG] Attempting to find files for hook code: {extracted_code}", "1;35"))
                found_files = get_ruff_files_for_code(extracted_code)
                if found_files:
                    target_files = found_files # Use all files found by ruff
                    # Limit context for prompt if too many files
                    if len(target_files) > 3:
                        prompt_context = f" in {target_files[0]}, {target_files[1]} and {len(target_files) - 2} other files"
                    else:
                         prompt_context = f" in {', '.join(target_files)}"
                    print(
                        colorize(
                            f"Found {len(target_files)} relevant file(s) by re-running ruff for {extracted_code}: {prompt_context}",
                            "1;32",
                        ),
                    )
                    # Flag that we now have target files
                    relevant_files = True # Set flag to proceed

            # If still no relevant files after trying ruff
            if not relevant_files:
                print(
                    colorize(
                        f"Skipping ambiguous hook issue: Cannot determine relevant file(s) for '{error_msg[:60]}...'",
                        "1;33",
                    ),
                )
                return None  # Signal that the issue was skipped
    else:
        # Fallback if no file and not a recognizable hook/class issue
        print(colorize("Cannot determine target file(s) for this issue.", "1;31"))
        return False

    # --- Limit files passed to Aider to prevent token limit issues ---
    MAX_FILES_FOR_AIDER = 10 # Configurable limit
    original_target_files_count = len(target_files)
    files_to_process = list(target_files) # Create a mutable copy

    if original_target_files_count > MAX_FILES_FOR_AIDER:
        print(
            colorize(
                f"Warning: Found {original_target_files_count} files for this issue. Processing only the first {MAX_FILES_FOR_AIDER} to avoid token limits.",
                "1;33",
            )
        )
        files_to_process = target_files[:MAX_FILES_FOR_AIDER]
        # Update prompt context to reflect the subset
        if len(files_to_process) > 1:
            prompt_context = f" in {files_to_process[0]} and {len(files_to_process) - 1} other files (subset of {original_target_files_count})"
        elif len(files_to_process) == 1:
             prompt_context = f" in {files_to_process[0]} (subset of {original_target_files_count})"

    # --- Print details ---
    if consolidated and file_path:  # Consolidated only makes sense with a file
        print(colorize(f"Fixing consolidated issues in {file_path}:", "1;36"))
        for line in error_msg.split("\n"):
            print(colorize(f"  - {line}", "1;37"))
    elif VERBOSE_MODE:
        print(colorize(f"Fixing issue{prompt_context}:", "1;36"))
        print(colorize(f"  {error_msg}", "1;37"))
    else:
        print(colorize(f"Fixing issue{prompt_context}...", "1;36"))

    try:
        # Check file existence only if a specific file is targeted
        if target_files and not all(os.path.exists(f) for f in target_files):
            missing = [f for f in target_files if not os.path.exists(f)]
            print(
                colorize(f"Target file(s) do not exist: {', '.join(missing)}", "1;31"),
            )
            return False

        # Check if any target files should be ignored based on gitignore patterns
        if target_files:
            ignored_files = [f for f in target_files if should_ignore_file(f)]
            if ignored_files:
                print(
                    colorize(
                        f"Skipping ignored file(s): {', '.join(ignored_files)}", "1;33",
                    ),
                )

                # If all files are ignored, return early
                if len(ignored_files) == len(target_files):
                    print(
                        colorize(
                            "All target files are in gitignore - skipping fix.", "1;33",
                        ),
                    )

                    # Move to RESOLVED to avoid repeated attempts
                    if issue and "line" in issue and not issue_moved:
                        if move_todo_issue(
                            issue["line"], ISSUES_HEADER, RESOLVED_HEADER,
                        ):
                            print(
                                colorize(
                                    f"Moved ignored file issue to {RESOLVED_HEADER}",
                                    "1;32",
                                ),
                            )
                            issue_moved = True

                    return True  # Return success to avoid repeated attempts

                # Remove ignored files from the target list
                target_files = [f for f in target_files if f not in ignored_files]

        # --- Preprocessing (only if we have a specific file) ---
        if target_files:
            # Check for syntax errors that might need preprocessing
            syntax_error = (
                "syntax error" in error_msg.lower()
                or "cannot parse" in error_msg.lower()
                or "invalid syntax" in error_msg.lower()
            )
            if syntax_error:
                # Try to fix syntax errors with preprocessing
                if preprocess_syntax_errors(target_files[0], error_msg):
                    print(
                        colorize(
                            "Syntax error potentially fixed with preprocessing.", "1;32",
                        ),
                    )
                    # Assume fixed for now, might need post-check
                    # We might want to move the TODO item here if successful
                    if issue and "line" in issue:
                        move_todo_issue(issue["line"], ISSUES_HEADER, RESOLVED_HEADER)
                    return True

            # Use ruff directly for simple fixes like D213 if file exists
            if "D213" in error_msg and postprocess_with_ruff(target_files[0]):
                print(colorize("D213 issue potentially fixed with ruff.", "1;32"))
                if issue and "line" in issue:
                    move_todo_issue(issue["line"], ISSUES_HEADER, RESOLVED_HEADER)
                return True

        # --- Run Aider ---
        with tempfile.NamedTemporaryFile(mode="w", delete=True) as f:
            # Explicitly unset OpenAI env vars to prevent interference with config files
            # Ensure Aider uses the config from ~/.aider.conf.yml or repo
            for key in ["OPENAI_API_KEY", "OPENAI_API_BASE", "OPENAI_ORGANIZATION"]:
                if key in os.environ:
                    del os.environ[key]
                    if VERBOSE_MODE:
                        print(colorize(f"Unset environment variable: {key}", "1;30"))

            # Set up aider environment
            os.environ["AIDER_NO_AUTO_COMMIT"] = "1"
            os.environ["AIDER_CHAT_HISTORY_FILE"] = f.name
            os.environ["AIDER_NO_INPUT"] = "1"
            os.environ["AIDER_QUIET"] = (
                "0" if VERBOSE_MODE else "1"
            )  # Show aider output if verbose

            # Set up environment for controlled pre-commit interaction
            restore_hook = setup_pre_commit_environment()

            # Track resources for proper cleanup
            main_model = None
            coder = None
            success = False

            try:
                # Create the model directly
                model_name = os.environ.get(
                    "AIDER_MODEL", "deepinfra/google/gemini-2.0-flash-001",
                )
                if VERBOSE_MODE:
                    print(colorize(f"Using model: {model_name}", "1;36"))
                main_model = Model(model_name)

                # Read file content if available for better context
                file_content = ""
                if target_files and os.path.exists(target_files[0]):
                    try:
                        with open(
                            target_files[0], encoding="utf-8", errors="ignore",
                        ) as f:
                            file_content = f.read()
                    except Exception as e:
                        if VERBOSE_MODE:
                            print(colorize(f"Could not read file content: {e}", "1;33"))

                # Build a more informative prompt with file content
                prompt = f"# TASK\nFix the following issue{prompt_context}:\n\n"
                prompt += f"## ERROR DESCRIPTION\n{error_msg}\n\n"
                
                # Add file analysis section
                prompt += "## FILE ANALYSIS\n"
                if file_content:
                    # Analyze code structure
                    imports = []
                    classes = []
                    functions = []
                    
                    # Extract simple structure using regex
                    imports = re.findall(r'^import\s+(.+)$|^from\s+(.+)\s+import', file_content, re.MULTILINE)
                    classes = re.findall(r'^class\s+(\w+)', file_content, re.MULTILINE)
                    functions = re.findall(r'^def\s+(\w+)', file_content, re.MULTILINE)
                    
                    # Add structure information
                    if imports:
                        prompt += f"- File has {len(imports)} import statements\n"
                    if classes:
                        prompt += f"- File contains {len(classes)} classes: {', '.join(classes)}\n"
                    if functions:
                        prompt += f"- File contains {len(functions)} functions\n"
                        
                    # Add error-specific analysis
                    if "D213" in error_msg:
                        # Look for docstrings with incorrect format
                        docstrings = re.findall(r'"""(.+?)"""', file_content, re.DOTALL)
                        if docstrings:
                            prompt += f"- Found {len(docstrings)} docstrings, likely with incorrect D213 formatting\n"
                    elif any(code in error_msg for code in ["D100", "D101", "D102", "D103"]):
                        # Check for missing docstrings based on error code
                        if "D100" in error_msg and '"""' not in file_content[:200]:
                            prompt += "- Missing module-level docstring at the top of the file\n"
                        if "D101" in error_msg and classes:
                            prompt += "- Some classes may be missing docstrings\n"
                        if "D102" in error_msg and functions:
                            prompt += "- Some functions may be missing docstrings\n"
                    elif "G004" in error_msg:
                        # Look for f-strings in logging calls
                        fstring_logs = re.findall(r'logging\.\w+\(f[\'"]', file_content)
                        if fstring_logs:
                            prompt += f"- Found {len(fstring_logs)} instances of f-strings in logging statements\n"
                    elif "BLE001" in error_msg:
                        # Look for generic exception handlers
                        blind_excepts = re.findall(r'except Exception', file_content)
                        if blind_excepts:
                            prompt += f"- Found {len(blind_excepts)} instances of generic 'except Exception' blocks\n"
                    elif "TRY400" in error_msg:
                        # Look for exceptions using logging.error instead of logging.exception
                        error_logs = re.findall(r'except.*?\n.*?logging\.error', file_content, re.DOTALL)
                        if error_logs:
                            prompt += f"- Found {len(error_logs)} instances of logging.error in exception handlers\n"
                    elif "I001" in error_msg:
                        # Analyze import ordering issues
                        import_lines = re.findall(r'^(import|from)\s.*$', file_content, re.MULTILINE)
                        if import_lines and len(import_lines) > 5:
                            # Check for potential grouping (simple approach)
                            std_lib = sum(1 for imp in import_lines if not '.' in imp.split(' ')[1])
                            third_party = sum(1 for imp in import_lines if '.' in imp.split(' ')[1] and not imp.startswith('from dewey'))
                            local = sum(1 for imp in import_lines if 'dewey' in imp)
                            prompt += f"- Found {len(import_lines)} imports: {std_lib} standard lib, {third_party} third-party, {local} local\n"
                            prompt += "- Imports need to be reordered: std lib, third-party, local modules\n"
                    elif "S101" in error_msg:
                        # Look for assert statements
                        asserts = re.findall(r'^\s*assert\s', file_content, re.MULTILINE)
                        if asserts:
                            prompt += f"- Found {len(asserts)} assert statements that should be replaced with proper validation\n"
                    elif "ANN" in error_msg:
                        # Look for missing type annotations
                        # Count functions with and without type annotations
                        func_defs = re.findall(r'def\s+\w+\((.*?)\)(\s*->.*?)?:', file_content, re.DOTALL)
                        if func_defs:
                            with_types = sum(1 for _, ret in func_defs if ret.strip())
                            without_types = len(func_defs) - with_types
                            prompt += f"- Found {len(func_defs)} function definitions: {with_types} with return types, {without_types} missing types\n"
                            prompt += "- Type annotations need to be added for parameters and return values\n"
                    elif "F401" in error_msg:
                        # Look for unused imports
                        import_items = re.findall(r'from\s+\w+\s+import\s+(.*?)$|import\s+(.*?)$', file_content, re.MULTILINE)
                        all_imports = []
                        for match in import_items:
                            items = match[0] or match[1]
                            if ',' in items:
                                all_imports.extend([i.strip() for i in items.split(',')])
                            else:
                                all_imports.append(items.strip())
                                
                        prompt += f"- Found {len(all_imports)} imported items, some may be unused\n"
                        prompt += "- Unused imports should be removed\n"
                            
                prompt += "\n## INSTRUCTIONS\n"

                # Add specific instructions for certain types of errors
                if issue and issue["type"] == "class" and class_name:
                    prompt += f"- Implement the required method in the '{class_name}' class.\n"
                    prompt += "- Make sure the method has the correct signature.\n"
                    prompt += "- Add proper docstrings.\n"
                elif "D213" in error_msg:
                    prompt += "- Fix docstring formatting: multi-line docstring summaries should start at the second line.\n"
                    prompt += '- Format should be:\n```\n"""\n<summary>\n\n<details>\n"""\n```\n'
                elif any(
                    code in error_msg for code in ["D100", "D101", "D102", "D103"]
                ):
                    prompt += "- Add missing docstrings following Google style.\n"
                    prompt += "- Include Args/Returns/Raises sections if applicable.\n"
                elif "TRY400" in error_msg:
                    prompt += "- Replace logging.error with logging.exception in exception handlers.\n"
                    prompt += (
                        "- This will automatically include the traceback in the log.\n"
                    )
                elif "G004" in error_msg:
                    prompt += "- Replace f-strings in logging calls with % formatting or .format().\n"
                    prompt += '- Example: Change logging.info(f"Found {count}") to logging.info("Found %s", count)\n'
                elif "BLE001" in error_msg:
                    prompt += "- Replace generic 'except Exception:' with specific exception types.\n"
                    prompt += "- Example: Change 'except Exception:' to 'except (ValueError, TypeError):'\n"
                elif "I001" in error_msg:
                    prompt += "- Fix import sorting: standard library first, then third-party, then local.\n"
                    prompt += "- Sort each group alphabetically.\n"
                elif "S101" in error_msg:
                    prompt += "- Replace assert statements with proper error handling using if/raise.\n"
                    prompt += "- Asserts can be disabled with the -O flag, so they're not reliable for runtime checks.\n"
                elif "ANN" in error_msg:
                    prompt += "- Add type annotations to function parameters and return values.\n"
                    prompt += "- Import needed types from typing module (List, Dict, Optional, etc.).\n"

                # Add the file content if available
                if file_content:
                    prompt += f"\n## FILE CONTENT\n```python\n{file_content}\n```\n"
                elif target_files:
                    prompt += f"\nNote: Unable to read file content for {target_files[0]}. This file exists but couldn't be read properly.\n"
                    prompt += f"Please fix the issue in {target_files[0]} based on the error description.\n"
                else:
                    prompt += "\nNote: No specific file could be identified. Please analyze the error and determine which file needs modification.\n"

                prompt += "\n## EXPECTED OUTPUT\n"
                prompt += (
                    "- Provide the COMPLETE fixed code (not just the changed part).\n"
                )
                prompt += (
                    "- DO NOT use placeholders or ... to represent unchanged code.\n"
                )
                prompt += (
                    "- The entire file needs to be properly formatted and fixed.\n"
                )
                prompt += "- Include ALL imports at the top of the file.\n"

                # Add reminder of which file we're working with
                if target_files:
                    prompt += f"\nRemember, you are fixing: {target_files[0]}\n"

                print(
                    colorize(
                        f"Running Aider with prompt: \n---\n{prompt[:300]}...\n---",
                        "1;30",
                    ),
                )

                # Create aider coder
                coder = Coder.create(
                    main_model=main_model,
                    fnames=target_files or None,
                    io=InputOutput(yes=True, input_history_file=f.name),
                    edit_format="diff",
                    map_tokens=1024,
                )

                # Run aider with error handling
                try:
                    coder.run(with_message=prompt)
                    success = True
                except Exception as e:
                    if "Unable to commit" in str(e) or "git commit" in str(e).lower():
                        # This is a git-related error we can safely ignore
                        if VERBOSE_MODE:
                            print(colorize(f"Ignoring git error: {e}", "1;33"))
                        # The changes should still have been applied to the file
                        print(
                            colorize(
                                "Git commit failed, but changes may have been applied",
                                "1;33",
                            ),
                        )
                        success = True
                    else:
                        # This is another kind of error, re-raise it
                        raise
            except Exception as e:
                print(colorize(f"Error during Aider execution: {e}", "1;31"))
                if VERBOSE_MODE:
                    import traceback

                    print(colorize(traceback.format_exc(), "1;31"))
                success = False
            finally:
                # Ensure proper cleanup of resources and restore environment
                restore_hook()

                # Clean up Aider resources with proper error handling
                if coder:
                    try:
                        del coder
                    except Exception as cleanup_error:
                        if VERBOSE_MODE:
                            print(
                                colorize(
                                    f"Error cleaning up coder: {cleanup_error}", "1;33",
                                ),
                            )

                if main_model:
                    try:
                        # Add small delay to allow any pending operations to complete
                        time.sleep(0.5)
                        del main_model
                    except Exception as cleanup_error:
                        if VERBOSE_MODE:
                            print(
                                colorize(
                                    f"Error cleaning up model: {cleanup_error}", "1;33",
                                ),
                            )

        # --- Postprocessing for successful Aider runs ---
        # Do post-processing with ruff BEFORE validation
        if success and target_files:
            if postprocess_with_ruff(target_files[0]):
                print(colorize("Post-processing with ruff successful.", "1;32"))
            else:
                print(colorize("Post-processing with ruff had issues.", "1;33"))
                # Continue anyway - validation will tell us if issues remain

        # Validate the fix if we have specific files
        fix_validated = False
        if target_files:
            print(colorize("Validating fix...", "1;36"))
            validation_results = [validate_fix(file_path) for file_path in target_files]
            fix_validated = all(validation_results)

            if fix_validated:
                print(
                    colorize(
                        "✅ Fix validation passed! The code now meets quality standards.",
                        "1;32",
                    ),
                )
            else:
                print(
                    colorize("⚠️ Fix validation failed. Some issues may remain.", "1;33"),
                )
                # If validation failed but we made changes, try one more aggressive fix
                if success:
                    print(
                        colorize(
                            "Attempting more aggressive fixing with unsafe fixes...",
                            "1;36",
                        ),
                    )
                    if postprocess_with_ruff(target_files[0]):
                        # Check if that fixed everything
                        if validate_fix(target_files[0]):
                            print(
                                colorize(
                                    "✅ Second validation passed after aggressive fixes!",
                                    "1;32",
                                ),
                            )
                            fix_validated = True
                        else:
                            print(
                                colorize("❌ Second validation still failed.", "1;33"),
                            )
                    else:
                        print(colorize("❌ Aggressive fixing failed too.", "1;33"))
        else:
            # For hook issues without specific files, we can't validate directly
            # Consider the fix tentatively successful
            fix_validated = (
                success  # Only consider it validated if aider ran successfully
            )
            print(colorize("ℹ️ No specific files to validate.", "1;36"))

        # --- Postprocessing (only if we have a specific file) ---
        if target_files:
            if postprocess_with_ruff(target_files[0]):
                print(colorize("Post-processing with ruff successful.", "1;32"))
            else:
                print(colorize("Post-processing with ruff had issues.", "1;33"))

        # Assume success if Aider ran without throwing an exception for now
        print(colorize("Aider fix attempt completed.", "1;32"))

        # Mark the issue as fixed in TODO.md
        if issue and "line" in issue and not issue_moved:
            # Only move to RESOLVED if validation passed
            if fix_validated and move_todo_issue(
                issue["line"], ISSUES_HEADER, RESOLVED_HEADER,
            ):
                print(colorize(f"Moved issue to {RESOLVED_HEADER}", "1;32"))
                issue_moved = True
            # If validation failed but we made changes, move to WIP
            elif not fix_validated and move_todo_issue(
                issue["line"], ISSUES_HEADER, WIP_HEADER,
            ):
                print(
                    colorize(
                        f"Validation failed; moved issue to {WIP_HEADER} for further attention",
                        "1;33",
                    ),
                )
                issue_moved = True
            else:
                print(colorize(f"Failed to move issue to {RESOLVED_HEADER}", "1;33"))

        # Return success only if validation passed
        return fix_validated

    except Exception as e:
        print(colorize(f"Error running aider: {e}", "1;31"))
        if VERBOSE_MODE:
            import traceback

            print(colorize(traceback.format_exc(), "1;31"))

        # Move to WIP if aider failed and in interactive mode
        if issue and "line" in issue and INTERACTIVE_MODE and not issue_moved:
            if input("Aider failed. Move issue to WIP section? (y/n): ").lower() == "y":
                move_issue_to_section(line, "## Work in Progress")
                issue_moved = True

        return False


def is_file_used_by_core_scripts(file_path: str) -> bool:
    """
    Check if a file is imported or used by core scripts in the project.

    Args:
    ----
        file_path: Path to the file to check

    Returns:
    -------
        True if the file is used by core scripts, False otherwise

    """
    # Check if the file exists
    if not os.path.exists(file_path):
        return False

    # Get the module name from the file path
    file_name = os.path.basename(file_path)
    module_name = os.path.splitext(file_name)[0]

    # List of core script directories to check
    core_dirs = ["app", "dewey", "lib", "src"]

    # Look for imports of this module in other files
    for core_dir in core_dirs:
        if not os.path.exists(core_dir):
            continue

        # Use grep to find imports of this module
        try:
            # Search for "import module" or "from module import"
            cmd = [
                "grep",
                "-r",
                f"(import {module_name}|from {module_name} import)",
                "--include=*.py",
                core_dir,
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)

            # If grep found matches, the file is used
            if result.returncode == 0 and result.stdout.strip():
                if VERBOSE_MODE:
                    print(f"File {file_path} is used by files in {core_dir}")
                return True
        except Exception as e:
            if VERBOSE_MODE:
                print(f"Error checking if {file_path} is used: {e}")

    # If we get here, the file is not used by core scripts
    if VERBOSE_MODE:
        print(f"File {file_path} is not used by core scripts")
    return False


def calculate_script_importance(file_path: str) -> float:
    """
    Calculate importance score of a script using multiple heuristics.

    Uses a combination of signals to determine a script's importance:
    - Import relationships
    - Configuration file references
    - Git activity and age
    - Entry point relationships
    - Function call relationships

    Args:
    ----
        file_path: Path to the script file to analyze

    Returns:
    -------
        A float from 0 to 100 representing the importance score
    """
    if not os.path.exists(file_path):
        return 0.0

    # Initialize scoring system
    score = 0.0
    max_possible_score = 5.0  # Total possible points

    # Extract file info
    file_name = os.path.basename(file_path)
    module_name = os.path.splitext(file_name)[0]

    # 1. Direct import relationships (similar to is_file_used_by_core_scripts but with scoring)
    import_score = _check_import_relationships(file_path, module_name)
    score += import_score * 1.0  # Weight: 1.0

    # 2. Check config file references
    config_score = _check_config_references(file_path, module_name)
    score += config_score * 1.5  # Weight: 1.5 - configuration is a strong signal

    # 3. Check git history - active files are more likely to be important
    activity_score = _check_git_activity(file_path)
    score += activity_score * 0.5  # Weight: 0.5

    # 4. Check for usage as entry points
    entry_score = _check_entry_points(file_path, module_name)
    score += entry_score * 1.0  # Weight: 1.0

    # 5. Check for functions being called from other modules
    call_score = _check_function_calls(file_path, module_name)
    score += call_score * 1.0  # Weight: 1.0

    # Normalize to 0-100 range
    normalized_score = min(score / max_possible_score * 100, 100.0)

    if VERBOSE_MODE:
        print(f"File {file_path} importance score: {normalized_score:.2f}")
        print(
            f"  Import: {import_score:.2f}, Config: {config_score:.2f}, Activity: {activity_score:.2f}",
        )
        print(f"  Entry: {entry_score:.2f}, Calls: {call_score:.2f}")

    return normalized_score


def _check_import_relationships(file_path: str, module_name: str) -> float:
    """
    Check how many files import this module.

    Args:
    ----
        file_path: Path to the file
        module_name: Module name

    Returns:
    -------
        Float score from 0.0 to 1.0

    """
    import_count = 0
    weight_by_dir = {
        "app": 1.0,  # Application code
        "dewey": 0.9,  # Core business logic
        "lib": 0.8,  # Library code
        "src": 0.7,  # Source files
        "scripts": 0.3,  # Utility scripts
        "tests": 0.2,  # Test files
    }

    # Core directories with higher importance
    core_dirs = ["app", "dewey", "lib", "src", "scripts", "tests"]

    # Check both direct imports and usage patterns
    for dir_name in core_dirs:
        if not os.path.exists(dir_name):
            continue

        dir_weight = weight_by_dir.get(dir_name, 0.5)

        # Check for direct imports
        try:
            # Regular import
            cmd1 = [
                "grep",
                "-r",
                f"import\\s+{module_name}\\b",
                "--include=*.py",
                dir_name,
            ]
            # From import
            cmd2 = [
                "grep",
                "-r",
                f"from\\s+{module_name}\\s+import",
                "--include=*.py",
                dir_name,
            ]
            # Relative import
            cmd3 = [
                "grep",
                "-r",
                f"from\\s+\\.{module_name}\\s+import",
                "--include=*.py",
                dir_name,
            ]

            for cmd in [cmd1, cmd2, cmd3]:
                result = subprocess.run(
                    cmd, capture_output=True, text=True, check=False,
                )
                if result.returncode == 0 and result.stdout.strip():
                    import_count += len(result.stdout.strip().split("\n")) * dir_weight
        except Exception as e:
            if VERBOSE_MODE:
                print(f"Error checking imports for {file_path}: {e}")

    # Normalize the score based on number of imports
    if import_count > 10:
        return 1.0
    if import_count > 5:
        return 0.8
    if import_count > 2:
        return 0.5
    if import_count > 0:
        return 0.3
    return 0.0


def _check_config_references(file_path: str, module_name: str) -> float:
    """
    Check if the module is referenced in config files.

    Args:
    ----
        file_path: Path to the file
        module_name: Module name

    Returns:
    -------
        Float score from 0.0 to 1.0

    """
    score = 0.0

    # Common configuration files
    config_files = [
        "dewey.yaml",
        "pyproject.toml",
        "setup.py",
        "setup.cfg",
        "requirements.txt",
        "Makefile",
        "dewey/config/dewey.yaml",
        ".pre-commit-config.yaml",
    ]

    for config_file in config_files:
        if not os.path.exists(config_file):
            continue

        try:
            with open(config_file, encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # Check for module name in config
            if module_name in content:
                # Weight by config file importance
                if (
                    config_file == "dewey.yaml"
                    or config_file == "dewey/config/dewey.yaml"
                ):
                    score += 0.8  # Main project config
                elif config_file in ["setup.py", "pyproject.toml"]:
                    score += 0.6  # Package definition
                else:
                    score += 0.3  # Other config

                if VERBOSE_MODE:
                    print(f"Module {module_name} referenced in {config_file}")
        except Exception as e:
            if VERBOSE_MODE:
                print(f"Error checking config references for {file_path}: {e}")

    # Cap at 1.0
    return min(score, 1.0)


def _check_git_activity(file_path: str) -> float:
    """
    Check the git activity of the file.

    Args:
    ----
        file_path: Path to the file

    Returns:
    -------
        Float score from 0.0 to 1.0

    """
    score = 0.0

    try:
        # Check if git is available
        subprocess.run(["git", "--version"], check=True, capture_output=True)

        # Check commit count for the file
        cmd_count = ["git", "rev-list", "--count", "HEAD", "--", file_path]
        result_count = subprocess.run(
            cmd_count, capture_output=True, text=True, check=False,
        )

        if result_count.returncode == 0 and result_count.stdout.strip():
            commit_count = int(result_count.stdout.strip())

            # Get file age (days since last commit)
            cmd_age = ["git", "log", "-1", "--format=%at", "--", file_path]
            result_age = subprocess.run(
                cmd_age, capture_output=True, text=True, check=False,
            )

            if result_age.returncode == 0 and result_age.stdout.strip():
                last_commit_timestamp = int(result_age.stdout.strip())
                current_timestamp = int(time.time())
                days_since_last_commit = (current_timestamp - last_commit_timestamp) / (
                    60 * 60 * 24
                )

                # Recent commits are more valuable
                if days_since_last_commit < 30:  # Within a month
                    recency_score = 0.3
                elif days_since_last_commit < 90:  # Within 3 months
                    recency_score = 0.2
                elif days_since_last_commit < 365:  # Within a year
                    recency_score = 0.1
                else:
                    recency_score = 0.0

                # More commits indicate more activity/importance
                if commit_count > 20:
                    frequency_score = 0.7
                elif commit_count > 10:
                    frequency_score = 0.5
                elif commit_count > 5:
                    frequency_score = 0.3
                elif commit_count > 0:
                    frequency_score = 0.1
                else:
                    frequency_score = 0.0

                score = recency_score + frequency_score

                if VERBOSE_MODE:
                    print(
                        f"Git stats for {file_path}: {commit_count} commits, last: {days_since_last_commit:.1f} days ago",
                    )
    except Exception as e:
        if VERBOSE_MODE:
            print(f"Error checking git activity for {file_path}: {e}")

    return min(score, 1.0)


def _check_entry_points(file_path: str, module_name: str) -> float:
    """
    Check if the module is used as an entry point.

    Args:
    ----
        file_path: Path to the file
        module_name: Module name

    Returns:
    -------
        Float score from 0.0 to 1.0

    """
    score = 0.0

    # Check if the file has a main function or is executable
    try:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # Check for main execution
        if "__main__" in content and "if __name__ == " in content:
            score += 0.3

        # Check for shebang line
        if content.startswith("#!/") or content.startswith("#! /"):
            score += 0.3

        # Check if file is executable
        if os.access(file_path, os.X_OK):
            score += 0.2
    except Exception as e:
        if VERBOSE_MODE:
            print(f"Error checking entry points for {file_path}: {e}")

    # Check if referenced in setup.py entry_points
    try:
        setup_files = ["setup.py", "pyproject.toml"]
        for setup_file in setup_files:
            if not os.path.exists(setup_file):
                continue

            with open(setup_file, encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # Check for entry points referencing this module
            if "entry_points" in content and module_name in content:
                score += 0.5
                break
    except Exception as e:
        if VERBOSE_MODE:
            print(f"Error checking setup entry points for {file_path}: {e}")

    return min(score, 1.0)


def _check_function_calls(file_path: str, module_name: str) -> float:
    """
    Check how the functions in this module are called by other modules.

    Args:
    ----
        file_path: Path to the file
        module_name: Module name

    Returns:
    -------
        Float score from 0.0 to 1.0

    """
    score = 0.0

    # Extract function names from the module
    functions = []
    try:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            content = f.readlines()

        for line in content:
            line = line.strip()
            if line.startswith("def ") and "(" in line:
                func_name = line[4 : line.index("(")]
                if not func_name.startswith("_"):  # Skip private functions
                    functions.append(func_name)
    except Exception as e:
        if VERBOSE_MODE:
            print(f"Error extracting functions from {file_path}: {e}")
        return 0.0

    if not functions:
        return 0.0

    # Check for function calls across the codebase
    call_count = 0
    core_dirs = ["app", "dewey", "lib", "src", "scripts"]

    for function in functions:
        for dir_name in core_dirs:
            if not os.path.exists(dir_name):
                continue

            try:
                # Pattern 1: module_name.function_name
                cmd1 = [
                    "grep",
                    "-r",
                    f"{module_name}\\.{function}\\b",
                    "--include=*.py",
                    dir_name,
                ]
                # Pattern 2: from module_name import ... function ...
                cmd2 = [
                    "grep",
                    "-r",
                    f"from\\s+{module_name}\\s+import\\s+.*\\b{function}\\b",
                    "--include=*.py",
                    dir_name,
                ]

                for cmd in [cmd1, cmd2]:
                    result = subprocess.run(
                        cmd, capture_output=True, text=True, check=False,
                    )
                    if result.returncode == 0 and result.stdout.strip():
                        matches = result.stdout.strip().split("\n")
                        # Filter out self-references
                        matches = [m for m in matches if file_path not in m]
                        call_count += len(matches)
            except Exception as e:
                if VERBOSE_MODE:
                    print(
                        f"Error checking function calls for {file_path}:{function}: {e}",
                    )

    # Score based on call count
    if call_count > 20:
        score = 1.0
    elif call_count > 10:
        score = 0.8
    elif call_count > 5:
        score = 0.6
    elif call_count > 2:
        score = 0.4
    elif call_count > 0:
        score = 0.2

    if VERBOSE_MODE and call_count > 0:
        print(f"Function call count for {file_path}: {call_count}")

    return score


def display_issues_menu(issues: list[dict[str, str]]) -> dict[str, Any]:
    """
    Display a menu of issues grouped by file for the user to select from.
    
    Args:
    ----
        issues: List of issues to display
        
    Returns:
    -------
        Selected option as a dictionary, or None if user chooses to exit
    """
    global MENU_SELECTIONS
    MENU_SELECTIONS = []  # Reset global menu selections
    
    if not issues:
        print(colorize("No issues found to process.", "1;33"))
        return None
        
    # Group issues by file, and separate hook and class issues
    issues_by_file = {}
    hook_issues = []
    class_issues = []
    
    for issue in issues:
        issue_type = issue.get("type", "file")
        file_path = issue.get("file_path", "")
        
        if issue_type == "hook":
            hook_issues.append(issue)
        elif issue_type == "class" and not file_path:
            class_issues.append(issue)
        elif file_path:
            if file_path not in issues_by_file:
                issues_by_file[file_path] = []
            issues_by_file[file_path].append(issue)
    
    # Calculate importance for all files
    file_importance = {}  # file_path -> score
    for file_path in issues_by_file:
        score = calculate_script_importance(file_path)
        file_importance[file_path] = score
    
    # Group files by importance classification
    critical_files = []
    important_files = []
    ancillary_files = []
    legacy_files = []
    
    for file_path, score in file_importance.items():
        if score >= 80:
            critical_files.append((file_path, score))
        elif score >= 50:
            important_files.append((file_path, score))
        elif score >= 20:
            ancillary_files.append((file_path, score))
        else:
            legacy_files.append((file_path, score))
    
    # Sort files within each category by score (highest first)
    critical_files.sort(key=lambda x: x[1], reverse=True)
    important_files.sort(key=lambda x: x[1], reverse=True)
    ancillary_files.sort(key=lambda x: x[1], reverse=True)
    legacy_files.sort(key=lambda x: x[1], reverse=True)
    
    # Count issues by importance category
    issue_counts = {
        "critical": sum(len(issues_by_file[file_path]) for file_path, _ in critical_files),
        "important": sum(len(issues_by_file[file_path]) for file_path, _ in important_files),
        "ancillary": sum(len(issues_by_file[file_path]) for file_path, _ in ancillary_files),
        "legacy": sum(len(issues_by_file[file_path]) for file_path, _ in legacy_files),
    }
    
    # Add hook and class issues to the appropriate category (typically "important")
    issue_counts["important"] += len(class_issues)
    issue_counts["ancillary"] += len(hook_issues)
    
    # Display the menu with files grouped by importance
    print(colorize(f"\n{'-' * 80}", "1;36"))
    print(
        colorize(
            f"Found {len(issues)} issues from '{ISSUES_HEADER}' to process",
            "1;36",
        ),
    )
    print(colorize(f"{'-' * 80}", "1;36"))
    
    menu_index = 1
    
    # BATCH OPTIONS section
    print(colorize("\nBATCH OPTIONS:", "1;35"))
    print(colorize(f"{menu_index}. Process ALL issues", "1;37"))
    MENU_SELECTIONS.append(("all", "true"))
    menu_index += 1
    
    print(colorize(f"{menu_index}. Process CRITICAL issues only", "1;37"))
    MENU_SELECTIONS.append(("critical_only", "true"))
    menu_index += 1
    
    print(colorize(f"{menu_index}. Process IMPORTANT issues only", "1;37"))
    MENU_SELECTIONS.append(("important_only", "true"))
    menu_index += 1
    
    print(colorize(f"{menu_index}. Generate LLM instructions", "1;37"))
    MENU_SELECTIONS.append(("generate_llm", "true"))
    menu_index += 1
    
    # Display files with issues by importance category
    if issue_counts["critical"] > 0:
        print(colorize("\nCRITICAL FILES:", "1;31"))
        for file_path, score in critical_files:
            file_issues = issues_by_file[file_path]
            print(colorize(f"{menu_index}. {file_path} ({len(file_issues)} issues, score: {score:.0f})", "1;31"))
            MENU_SELECTIONS.append(("file_group", {
                "file_path": file_path, 
                "issues": file_issues, 
                "importance": score
            }))
            menu_index += 1
    
    if issue_counts["important"] > 0:
        print(colorize("\nIMPORTANT FILES:", "1;33"))
        
        # Display file issues
        for file_path, score in important_files:
            file_issues = issues_by_file[file_path]
            print(colorize(f"{menu_index}. {file_path} ({len(file_issues)} issues, score: {score:.0f})", "1;33"))
            MENU_SELECTIONS.append(("file_group", {
                "file_path": file_path, 
                "issues": file_issues, 
                "importance": score
            }))
            menu_index += 1
        
        # Display class issues grouped by class name
        if class_issues:
            print(colorize("\nCLASS ISSUES:", "1;35"))
            classes_by_name = {}
            for issue in class_issues:
                class_match = re.search(r"Class '([^']+)'", issue.get("line", ""))
                if class_match:
                    class_name = class_match.group(1)
                    if class_name not in classes_by_name:
                        classes_by_name[class_name] = []
                    classes_by_name[class_name].append(issue)
            
            for class_name, class_issues_list in classes_by_name.items():
                print(colorize(f"{menu_index}. Class '{class_name}' ({len(class_issues_list)} issues)", "1;35"))
                MENU_SELECTIONS.append(("class_group", {
                    "class_name": class_name, 
                    "issues": class_issues_list
                }))
                menu_index += 1
    
    if issue_counts["ancillary"] > 0:
        print(colorize("\nANCILLARY FILES:", "1;32"))
        
        # Display file issues
        for file_path, score in ancillary_files:
            file_issues = issues_by_file[file_path]
            print(colorize(f"{menu_index}. {file_path} ({len(file_issues)} issues, score: {score:.0f})", "1;32"))
            MENU_SELECTIONS.append(("file_group", {
                "file_path": file_path, 
                "issues": file_issues, 
                "importance": score
            }))
            menu_index += 1
        
        # Display hook issues grouped by hook type
        if hook_issues:
            print(colorize("\nHOOK ISSUES:", "1;34"))
            hooks_by_type = {}
            for issue in hook_issues:
                hook_line = issue.get("line", "")
                # Try to identify hook type (ruff, flake8, etc.)
                hook_type = "general"
                for hook_name in ["ruff", "flake8", "mypy", "black", "isort", "pytest"]:
                    if hook_name in hook_line.lower():
                        hook_type = hook_name
                        break
                
                if hook_type not in hooks_by_type:
                    hooks_by_type[hook_type] = []
                hooks_by_type[hook_type].append(issue)
            
            for hook_type, hook_issues_list in hooks_by_type.items():
                print(colorize(f"{menu_index}. {hook_type.upper()} linter issues ({len(hook_issues_list)} issues)", "1;34"))
                MENU_SELECTIONS.append(("hook_group", {
                    "hook_type": hook_type, 
                    "issues": hook_issues_list
                }))
                menu_index += 1
    
    if issue_counts["legacy"] > 0:
        print(colorize("\nLEGACY FILES:", "1;36"))
        for file_path, score in legacy_files:
            file_issues = issues_by_file[file_path]
            print(colorize(f"{menu_index}. {file_path} ({len(file_issues)} issues, score: {score:.0f})", "1;36"))
            MENU_SELECTIONS.append(("file_group", {
                "file_path": file_path, 
                "issues": file_issues, 
                "importance": score
            }))
            menu_index += 1
    
    # Get user selection
    print(colorize("\nEnter the number of your selection, or 'q' to quit: ", "1;37"), end="")
    selection = input().strip()
    
    if selection.lower() in ('q', 'quit', 'exit'):
        return None
    
    try:
        item_number = int(selection)
        if 1 <= item_number < menu_index:
            selection_type, selection_value = MENU_SELECTIONS[item_number - 1]
            
            if selection_type == "all":
                return {"all": "true"}
            elif selection_type == "critical_only":
                return {"critical_only": "true"}
            elif selection_type == "important_only":
                return {"important_only": "true"}
            elif selection_type == "generate_llm":
                return {"generate_llm": "true"}
            elif selection_type == "file_group":
                return {
                    "type": "file_group",
                    "file_path": selection_value["file_path"],
                    "issues": selection_value["issues"],
                    "importance": selection_value["importance"]
                }
            elif selection_type == "class_group":
                return {
                    "type": "class_group",
                    "class_name": selection_value["class_name"],
                    "issues": selection_value["issues"]
                }
            elif selection_type == "hook_group":
                return {
                    "type": "hook_group",
                    "hook_type": selection_value["hook_type"],
                    "issues": selection_value["issues"]
                }
        else:
            print(colorize("Invalid selection. Please try again.", "1;31"))
            return display_issues_menu(issues)  # Recursive call to get valid input
    except ValueError:
        print(colorize("Invalid input. Please enter a number or 'q' to quit.", "1;31"))
        return display_issues_menu(issues)  # Recursive call to get valid input


def process_all_issues(issues: list[dict[str, str]], filter_category: str = None) -> int:
    """
    Process all issues in batch mode, optionally filtering by category.
    
    Args:
    ----
        issues: List of issues to process
        filter_category: Optional category to filter issues by ('critical', 'important', etc.)
        
    Returns:
    -------
        Number of issues successfully processed
    """
    print(colorize(f"Processing issues in batch mode{'.' if not filter_category else f', filtering by {filter_category}.'}", "1;36"))
    
    # Group issues by category if filtering is needed
    if filter_category:
        # Build file groups first
        issues_by_file = {}
        class_issues = []
        hook_issues = []
        
        for issue in issues:
            issue_type = issue.get("type", "file")
            file_path = issue.get("file_path", "")
            
            if issue_type == "hook":
                hook_issues.append(issue)
            elif issue_type == "class" and not file_path:
                class_issues.append(issue)
            elif file_path:
                if file_path not in issues_by_file:
                    issues_by_file[file_path] = []
                issues_by_file[file_path].append(issue)
        
        # Calculate importance for all files
        filtered_issues = []
        
        # Apply filtering based on category
        if filter_category == "critical":
            # Include only critical files
            for file_path, file_issues in issues_by_file.items():
                score = calculate_script_importance(file_path)
                if score >= 80:  # Critical threshold
                    filtered_issues.extend(file_issues)
            
        elif filter_category == "important":
            # Include important files and class issues
            for file_path, file_issues in issues_by_file.items():
                score = calculate_script_importance(file_path)
                if score >= 50:  # Important threshold
                    filtered_issues.extend(file_issues)
            
            # Class issues are typically important
            filtered_issues.extend(class_issues)
            
        else:
            # Unknown category, just process all
            print(colorize(f"Unknown category '{filter_category}', processing all issues", "1;33"))
            filtered_issues = issues
            
        # Use the filtered issues
        issues_to_process = filtered_issues
    else:
        # No filtering, process all issues
        issues_to_process = issues
    
    # Process all selected issues
    successful = 0
    total = len(issues_to_process)
    
    print(colorize(f"Processing {total} issues...", "1;36"))
    
    for i, issue in enumerate(issues_to_process, 1):
        print(colorize(f"\nProcessing issue {i}/{total}", "1;36"))
        
        # Create a selection dictionary compatible with process_single_issue
        selection = {
            "type": issue.get("type", "file"),
            "file_path": issue.get("file_path", ""),
            "error_msg": issue.get("error_msg", ""),
            "line": issue.get("line", ""),
        }
        
        # For class issues, try to extract class name
        if selection["type"] == "class":
            class_match = re.search(r"Class '([^']+)'", issue.get("line", ""))
            if class_match:
                selection["class_name"] = class_match.group(1)
        
        # Process the issue
        process_single_issue(selection)
        successful += 1
    
    print(colorize(f"\nCompleted batch processing: {successful}/{total} issues processed", "1;32"))
    return successful

def parse_args() -> argparse.Namespace:
    """Parse command line arguments for quick_fix.py."""
    parser = argparse.ArgumentParser(
        description="Fix pre-commit issues interactively or in batch mode",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    parser.add_argument(
        "--batch",
        action="store_true",
        help="Run in batch mode (process all issues without interaction)",
    )
    
    parser.add_argument(
        "--category",
        type=str,
        choices=["critical", "important", "all"],
        default="all",
        help="Filter issues by category in batch mode",
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Show verbose output",
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Show debug information",
    )
    
    parser.add_argument(
        "--skip-capture",
        action="store_true",
        help="Skip running capture_precommit_issues.py",
    )
    
    parser.add_argument(
        "--generate-llm",
        action="store_true",
        help="Generate LLM instructions in fix_instructions.md",
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="fix_instructions.md",
        help="Output file for LLM instructions (default: fix_instructions.md)",
    )
    
    parser.add_argument(
        "--check-ignored",
        action="store_true",
        help="Check ignored files also",
    )
    
    parser.add_argument(
        "--generate-index",
        action="store_true",
        help="Generate codebase index using repomix",
    )
    
    parser.add_argument(
        "--raw-output",
        type=str,
        default="precommit_output.log",
        help="Path to raw pre-commit output file (default: precommit_output.log)",
    )
    
    parser.add_argument(
        "--use-gemini",
        action="store_true",
        help="Use Gemini to analyze pre-commit output",
    )
    
    return parser.parse_args()

def main():
    """Run the main logic of the quick_fix.py script."""
    global VERBOSE_MODE
    global DEBUG_MODE
    global INTERACTIVE_MODE
    global GENERATE_CODEBASE_INDEX
    global GENERATE_LLM_INSTRUCTIONS, LLM_INSTRUCTION_FILE
    global CHECK_IGNORED_FILES
    
    # Parse command line arguments
    args = parse_args()
    
    # Set global flags based on arguments
    VERBOSE_MODE = args.verbose or args.debug
    DEBUG_MODE = args.debug
    INTERACTIVE_MODE = not args.batch
    GENERATE_CODEBASE_INDEX = args.generate_index
    GENERATE_LLM_INSTRUCTIONS = args.generate_llm
    LLM_INSTRUCTION_FILE = args.output
    CHECK_IGNORED_FILES = args.check_ignored
    
    # Show current working directory
    print(colorize(f"Working directory: {os.getcwd()}", "1;36"))
    
    if GENERATE_CODEBASE_INDEX:
        print(colorize("Generating codebase index...", "1;34"))
        if not generate_codebase_index():
            print(colorize("Failed to generate codebase index.", "1;31"))
        sys.exit(0)
    
    # Handle Gemini analysis if requested
    if args.use_gemini:
        print(colorize("Using Gemini to analyze pre-commit output", "1;34"))
        if not args.skip_capture:
            print(colorize("Running pre-commit to capture issues...", "1;36"))
            # Run pre-commit with all hooks on all files and capture output
            raw_output_path = args.raw_output
            with open(raw_output_path, "w", encoding="utf-8") as output_file:
                try:
                    subprocess.run(
                        ["pre-commit", "run", "--all-files"],
                        stdout=output_file,
                        stderr=subprocess.STDOUT,
                        check=False,
                        encoding="utf-8",
                    )
                    print(
                        colorize(
                            f"Pre-commit output captured in {raw_output_path}", "1;32",
                        ),
                    )
                except Exception as e:
                    print(colorize(f"Error running pre-commit: {e}", "1;31"))
                    return
        
        # Process the raw output with Gemini
        analysis_file = process_precommit_with_llm(args.raw_output)
        if analysis_file:
            print(
                colorize(
                    f"Analysis complete! Results written to {analysis_file}", "1;32",
                ),
            )
            print(colorize(f"To view: cat {analysis_file}", "1;36"))
        sys.exit(0)
    
    if GENERATE_LLM_INSTRUCTIONS:
        print(colorize("Generating LLM instructions report...", "1;34"))
        try:
            # Make sure issues are loaded
            issues = read_todo_issues()
            
            if not issues:
                print(colorize("No issues found in TODO.md. Cannot generate LLM instructions.", "1;33"))
                sys.exit(1)
            
            generate_llm_instructions(issues)
            print(colorize(f"LLM instructions successfully generated in {LLM_INSTRUCTION_FILE}", "1;32"))
            sys.exit(0)
        except Exception as e:
            print(colorize(f"Unhandled error: {e}", "1;31"))
            print("Try running with --verbose for more information")
            if DEBUG_MODE:
                import traceback
                traceback.print_exc()
            sys.exit(1)
    
    # Run the capture script if needed
    if not args.skip_capture:
        run_capture_script()
    
    # Ensure TODO.md has the required sections
    ensure_todo_sections()
    
    # Read issues from TODO.md
    if VERBOSE_MODE:
        print(colorize("Reading issues from TODO.md...", "1;34"))
    issues = read_todo_issues()
    
    if not issues:
        print(colorize("No issues found in TODO.md", "1;33"))
        return
    
    print(colorize(f"Found {len(issues)} issues in TODO.md", "1;32"))
    
    # Handle different modes
    if args.batch:
        # Batch mode
        print(colorize("Batch mode enabled", "1;32"))
        filter_cat = args.category if args.category != "all" else None
        if filter_cat:
            print(
                colorize(f"Filtering: Fixing only {filter_cat.upper()} issues", "1;33"),
            )
        else:
            print(colorize("Processing ALL categories", "1;32"))
        process_all_issues(issues, filter_cat)
    else:
        # Interactive menu mode
        print(colorize("Interactive mode: Choose an action", "1;34"))
        selection = display_issues_menu(issues)
        if selection is None:
            return  # User chose to exit
        
        if selection.get("all") == "true":
            process_all_issues(issues, None)
        elif selection.get("critical_only") == "true":
            process_all_issues(issues, "critical")
        elif selection.get("important_only") == "true":
            process_all_issues(issues, "important")
        elif selection.get("generate_llm") == "true":
            generate_llm_instructions(issues)
        else:
            # Process single selected issue
            process_single_issue(selection)

def run_capture_script() -> bool:
    """
    Run the capture_precommit_issues.py script to refresh the TODO.md file.
    
    This ensures we have the latest issues before attempting to fix them.
    
    Returns
    -------
        True if the script ran successfully, False otherwise
    """
    capture_script = os.path.join("scripts", "capture_precommit_issues.py")
    
    if not os.path.exists(capture_script):
        print(colorize(f"Error: Could not find {capture_script}", "1;31"))
        return False
    
    print(colorize("Capturing latest pre-commit issues...", "1;36"))
    
    try:
        cmd = [sys.executable, capture_script]
        if VERBOSE_MODE:
            cmd.append("--verbose")
        if DEBUG_MODE:
            cmd.append("--debug")
            
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        
        if result.returncode == 0:
            print(colorize("Successfully captured latest issues to TODO.md", "1;32"))
            if VERBOSE_MODE and result.stdout:
                print(colorize(result.stdout, "1;37"))
            return True
        
        print(colorize(f"Error running capture script: {result.stderr}", "1;31"))
        return False
    except Exception as e:
        print(colorize(f"Exception running capture script: {e}", "1;31"))
        return False

def get_user_input(prompt: str, default: str = "") -> str:
    """
    Get input from the user with a prompt and optional default value.
    
    Args:
    ----
        prompt: The prompt to display to the user
        default: The default value to return if the user enters nothing
        
    Returns:
    -------
        The user's input, or the default value if the user enters nothing
    """
    # If non-interactive mode, return default
    if not INTERACTIVE_MODE:
        if DEBUG_MODE:
            print(colorize(f"[DEBUG] Non-interactive mode, returning default: {default}", "1;35"))
        return default
    
    try:
        if default:
            user_input = input(colorize(f"{prompt} [{default}] ", "1;34")) or default
        else:
            user_input = input(colorize(prompt, "1;34"))
        return user_input
    except (KeyboardInterrupt, EOFError):
        print()  # Print a newline for better formatting
        return default

def process_single_issue(selection: dict[str, Any]) -> bool:
    """
    Process a single issue or a group of issues when selected from the menu.
    
    Args:
    ----
        selection: Dictionary containing information about the selected issue
        
    Returns:
    -------
        True if processing succeeded, False otherwise
    """
    try:
        # Extract information from the selection
        issue_type = selection.get("type", "file")
        file_path = selection.get("file_path", "")
        error_msg = selection.get("error_msg", "")
        line = selection.get("line", "")
        
        # Check for different issue types
        if issue_type == "file":
            # Check if this is a file group (multiple issues for one file)
            if isinstance(error_msg, list) or "\n" in error_msg:
                # Handle file group - multiple errors for the same file
                errors = error_msg.split("\n") if isinstance(error_msg, str) else error_msg
                
                # Make sure the file exists
                if not os.path.exists(file_path):
                    print(colorize(f"File not found: {file_path}", "1;31"))
                    return False
                
                print(colorize(f"Processing file group for: {file_path}", "1;36"))
                print(colorize(f"Found {len(errors)} issues to fix", "1;36"))
                
                # Run aider on this file with all errors
                consolidated_error_msg = "\n".join(errors) if isinstance(errors, list) else error_msg
                success = run_aider(
                    file_path=file_path,
                    error_msg=consolidated_error_msg,
                    issue=selection,
                    consolidated=True,
                )
                
                if success:
                    # Ask user if they want to move this issue to WIP or resolved
                    user_input = get_user_input(
                        prompt="Move this issue to WIP (w) or RESOLVED (r) section? (w/r/n): ",
                        default="n",
                    )
                    
                    if user_input.lower() == "w":
                        move_issue_to_section(line, "## Work in Progress")
                    elif user_input.lower() == "r":
                        move_issue_to_section(line, "## Resolved Issues")
                
                return success
            else:
                # Single file issue
                if not file_path:
                    print(colorize("No file path specified for file issue", "1;31"))
                    return False
                
                print(colorize(f"Processing single file issue: {file_path}", "1;36"))
                print(colorize(f"Error message: {error_msg}", "1;36"))
                
                success = run_aider(
                    file_path=file_path,
                    error_msg=error_msg,
                    issue=selection,
                )
                
                if success:
                    # Ask user if they want to move this issue to WIP or resolved
                    user_input = get_user_input(
                        prompt="Move this issue to WIP (w) or RESOLVED (r) section? (w/r/n): ",
                        default="n",
                    )
                    
                    if user_input.lower() == "w":
                        move_issue_to_section(line, "## Work in Progress")
                    elif user_input.lower() == "r":
                        move_issue_to_section(line, "## Resolved Issues")
                
                return success
        
        elif issue_type == "class":
            # Handle class implementation issues
            class_name = selection.get("class_name", "")
            if not class_name:
                print(colorize("No class name specified for class issue", "1;31"))
                return False
            
            print(colorize(f"Processing class issue: {class_name}", "1;36"))
            print(colorize(f"Error message: {error_msg}", "1;36"))
            
            success = run_aider(
                file_path=file_path,  # This might be empty for class issues
                error_msg=error_msg,
                issue=selection,
                class_name=class_name,
            )
            
            if success:
                # Ask user if they want to move this issue to WIP or resolved
                user_input = get_user_input(
                    prompt="Move this issue to WIP (w) or RESOLVED (r) section? (w/r/n): ",
                    default="n",
                )
                
                if user_input.lower() == "w":
                    move_issue_to_section(line, "## Work in Progress")
                elif user_input.lower() == "r":
                    move_issue_to_section(line, "## Resolved Issues")
            
            return success
        
        elif issue_type == "hook":
            # Handle hook issues
            hook_name = file_path  # For hook issues, the "file_path" is actually the hook name
            
            print(colorize(f"Processing hook issue: {hook_name}", "1;36"))
            print(colorize(f"Error message: {error_msg}", "1;36"))
            
            success = run_aider(
                file_path=None,  # No specific file for hook issues
                error_msg=error_msg,
                issue=selection,
            )
            
            # Check if the issue was skipped
            if success is None:
                print(colorize(f"Hook issue skipped due to ambiguity: {hook_name}", "1;33"))
                return True  # Return True to indicate it was handled (skipped)
            
            if success:
                # Ask user if they want to move this issue to WIP or resolved
                user_input = get_user_input(
                    prompt="Move this issue to WIP (w) or RESOLVED (r) section? (w/r/n): ",
                    default="n",
                )
                
                if user_input.lower() == "w":
                    move_issue_to_section(line, "## Work in Progress")
                elif user_input.lower() == "r":
                    move_issue_to_section(line, "## Resolved Issues")
            
            return success
        
        else:
            print(colorize(f"Unknown issue type: {issue_type}", "1;31"))
            return False
            
    except Exception as e:
        import traceback
        print(colorize(f"Error processing issue: {e}\n{traceback.format_exc()}", "1;31"))
        return False

def get_ruff_files_for_code(code: str) -> list[str] | None:
    """Run ruff check for a specific code and return affected files."""
    if not code or not re.match(r"^[A-Z]{1,4}\d{3}$", code):
        print(colorize(f"[DEBUG] Invalid or non-standard ruff code: {code}", "1;35"))
        return None

    command = ["ruff", "check", ".", "--select", code, "--output-format=json", "--exit-zero", "--no-cache"]
    print(colorize(f"[DEBUG] Re-running check: {' '.join(command)}", "1;35"))
    try:
        # Use shell=False for security and better argument handling
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=False,  # Don't raise exception on non-zero exit
            encoding='utf-8'
        )

        if result.returncode != 0 and "command not found" in result.stderr.lower():
             print(colorize(f"[DEBUG] 'ruff' command not found. Is ruff installed and in PATH?", "1;31"))
             return None # Ruff not found

        if result.stderr:
            # Print stderr only if it's not the typical "X fixable with --fix" message
            if "fixable with" not in result.stderr:
                 print(colorize(f"[DEBUG] Ruff stderr: {result.stderr.strip()}", "1;35"))

        # Even if ruff returns non-zero (issues found), proceed to parse JSON
        try:
            ruff_output = json.loads(result.stdout)
            files = sorted(list(set(item["filename"] for item in ruff_output)))
            print(colorize(f"[DEBUG] Ruff found {len(files)} files for code {code}", "1;35"))
            return files
        except json.JSONDecodeError:
            print(colorize(f"[DEBUG] Failed to decode Ruff JSON output for code {code}. Output was:\n{result.stdout}", "1;31"))
            return None # JSON parsing failed
        except Exception as e:
             print(colorize(f"[DEBUG] Error processing ruff output: {e}", "1;31"))
             return None # Other parsing errors

    except FileNotFoundError:
        print(colorize("[DEBUG] 'ruff' command not found. Is ruff installed and in PATH?", "1;31"))
        return None
    except Exception as e:
        print(colorize(f"[DEBUG] Error running ruff command: {e}", "1;31"))
        return None

def generate_llm_instructions(issues: list[dict[str, str]]) -> None:
    """
    Generate instructions for an LLM to fix issues file by file.

    Args:
    ----
        issues: List of issues to fix

    """
    print(colorize(f"Generating LLM instructions in {LLM_INSTRUCTION_FILE}...", "1;36"))

    # Group issues by file, and separate hook and class issues
    issues_by_file = {}
    hook_issues = []
    class_issues = []

    for issue in issues:
        issue_type = issue.get("type", "file")
        file_path = issue.get("file_path", "")

        if issue_type == "hook":
            hook_issues.append(issue)
        elif issue_type == "class" and not file_path:
            class_issues.append(issue)
        elif file_path:
            if file_path not in issues_by_file:
                issues_by_file[file_path] = []
            issues_by_file[file_path].append(issue)

    # Calculate importance for all files
    file_importance = {}
    for file_path in issues_by_file:
        score = calculate_script_importance(file_path)
        file_importance[file_path] = score

    # Group files by importance classification
    importance_categories = {
        "critical": [],
        "important": [],
        "ancillary": [],
        "legacy": [],
    }
    for file_path, score in file_importance.items():
        if score >= 80:
            importance_categories["critical"].append(file_path)
        elif score >= 50:
            importance_categories["important"].append(file_path)
        elif score >= 20:
            importance_categories["ancillary"].append(file_path)
        else:
            importance_categories["legacy"].append(file_path)

    # Group hook issues by type (e.g., D100, G004)
    hooks_by_type = {}
    for issue in hook_issues:
        hook_name = issue.get("file_path", "GENERAL") # Use file_path as hook name
        code_match = re.search(r"([A-Z]{1,4}\d{3})", hook_name)
        hook_type = code_match.group(1) if code_match else "GENERAL"
        if hook_type not in hooks_by_type:
            hooks_by_type[hook_type] = []
        hooks_by_type[hook_type].append(issue)

    # Create the instructions file content
    content = "# LLM Instructions for Fixing Pre-commit Issues\n\n"
    content += "This document provides instructions for fixing pre-commit issues identified in the codebase. Issues are grouped by file importance and hook type.\n\n"

    # Add Hook Issues section
    if hooks_by_type:
        content += "## Hook-Based Issues\n\n"
        content += "These issues are often reported by linters without a specific file path or span multiple files. Address them based on the error code.\n\n"
        for hook_type, type_issues in sorted(hooks_by_type.items()):
            content += f"### {hook_type} Issues ({len(type_issues)} instances)\n\n"
            # Add a generic instruction based on common codes (can be expanded)
            if hook_type == "D100":
                content += "- **Instruction:** Add missing docstrings to public modules.\n"
            elif hook_type == "G004":
                content += "- **Instruction:** Replace f-strings in logging calls with % formatting.\n"
            elif hook_type == "TRY400":
                 content += "- **Instruction:** Use `logging.exception` instead of `logging.error` in `except` blocks.\n"
            # Add more specific instructions here based on hook_type
            else:
                content += f"- **Instruction:** Review and fix all instances of {hook_type} violations reported by the linter.\n"
            
            # List a few examples
            content += "- **Examples:**\n"
            for i, issue in enumerate(type_issues):
                 if i >= 3: # Limit examples
                     content += f"  - ... and {len(type_issues) - 3} more\n"
                     break
                 error_msg = issue.get('error_msg', 'N/A').replace('\n', ' ').strip()
                 content += f"  - `{error_msg}`\n"
            content += "\n"

    # Add File-Based Issues section
    content += "## File-Based Issues\n\n"
    content += "Fix the issues listed for each file below.\n\n"

    # Write instructions by importance category
    for category, files in importance_categories.items():
        if not files:
            continue

        # Filter to files that have issues
        files_with_issues = sorted([file for file in files if file in issues_by_file], key=lambda f: file_importance[f], reverse=True)
        if not files_with_issues:
             continue

        content += f"### {category.upper()} FILES\n\n"
        for file_path in files_with_issues:
            score = file_importance[file_path]
            content += f"#### File: `{file_path}` (Importance: {score:.0f})\n\n"
            content += "**Issues:**\n"
            file_issues = issues_by_file[file_path]
            for issue in file_issues:
                error_msg = issue.get("error_msg", "N/A").strip()
                # Add line number if available from the original line
                line_num_match = re.search(r"\(line (\d+)\)", issue.get("line", ""))
                line_info = f" (line {line_num_match.group(1)})" if line_num_match else ""
                
                # Handle consolidated errors
                if issue.get("consolidated", False) and isinstance(error_msg, str):
                     content += f"- Consolidated issues:\n"
                     sub_issues = error_msg.split('\n')
                     for sub_issue in sub_issues:
                         content += f"  - `{sub_issue.strip()}`\n"
                else:
                    content += f"- `{error_msg}`{line_info}\n"
            content += "\n"

    # Add Class Issues section
    if class_issues:
        content += "## Class Implementation Issues\n\n"
        for issue in class_issues:
            error_msg = issue.get("error_msg", "N/A")
            class_match = re.search(r"Class '([^']+)'", error_msg)
            class_name = f"`{class_match.group(1)}`" if class_match else "Unknown Class"
            content += f"- **Class:** {class_name}\n"
            content += f"  - **Issue:** `{error_msg}`\n"
            content += f"  - **Instruction:** Implement the required method(s) in this class. Ensure correct signature and add docstrings.\n"
        content += "\n"

    # Write the content to the file
    try:
        with open(LLM_INSTRUCTION_FILE, "w", encoding="utf-8") as f:
            f.write(content)
        print(colorize(f"Instructions written to {LLM_INSTRUCTION_FILE}", "1;32"))
    except Exception as e:
        print(colorize(f"Error writing instructions file: {e}", "1;31"))

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(colorize("\nOperation cancelled by user", "1;33"))
    except Exception as e:
        print(colorize(f"Unhandled error: {e}", "1;31"))
        print(colorize("Try running with --verbose for more information", "1;33"))
        # Print the full traceback in debug mode
        import traceback
        traceback.print_exc()
        sys.exit(1)